{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset_url=\"graph_data/allmv_tmdb/allmv/graphsage/\"\n",
    "target_dataset_url=\"graph_data/allmv_tmdb/tmdb/graphsage/\"\n",
    "groundtruth_url=\"graph_data/allmv_tmdb/dictionaries/groundtruth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset_URL=\"graph_data/douban/online/graphsage/\"\n",
    "target_dataset_URL=\"graph_data/douban/offline/graphsage/\"\n",
    "groundtruth=\"graph_data/douban/dictionaries/groundtruth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "acm_dataset_url = \"graph_data/acm_dblp/acm/graphsage/\"\n",
    "dblp_dataset_url = \"graph_data/acm_dblp/dblp/graphsage/\"\n",
    "groundtruth=\"graph_data/acm_dblp/dictionaries/groundtruth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/data/private/interactive4388/bowen/.conda/envs/main/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import utils.graph_utils as graph_utils\n",
    "import json\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "from input.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "- Nodes:  3906\n",
      "- Edges:  8164\n",
      "Dataset info:\n",
      "- Nodes:  1118\n",
      "- Edges:  1511\n"
     ]
    }
   ],
   "source": [
    "source_dataset = Dataset(source_dataset_URL)\n",
    "target_dataset = Dataset(target_dataset_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "- Nodes:  9872\n",
      "- Edges:  39561\n",
      "Dataset info:\n",
      "- Nodes:  9916\n",
      "- Edges:  44808\n"
     ]
    }
   ],
   "source": [
    "acm_dataset = Dataset(acm_dataset_url)\n",
    "dblp_dataset = Dataset(dblp_dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6356, 0.4376, 0.1667,  ..., 0.1459, 0.0104, 0.2292],\n",
       "        [0.7368, 0.2456, 0.1637,  ..., 0.1501, 0.0000, 0.2183],\n",
       "        [0.4805, 0.5428, 0.0801,  ..., 0.0890, 0.0000, 0.3470],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = torch.FloatTensor(acm_dataset.features)\n",
    "F.normalize(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset.get_adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1],\n",
       "       [   0,    2],\n",
       "       [   1, 1272],\n",
       "       ...,\n",
       "       [3859, 3858],\n",
       "       [3881, 3880],\n",
       "       [3897, 3896]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset.get_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acm_dataset.get_adjacency_matrix())[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acm_dataset.get_edges()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def load_gt(path, id2idx_src=None, id2idx_trg=None, format='matrix'):    \n",
    "    if id2idx_src:\n",
    "        conversion_src = type(list(id2idx_src.keys())[0])\n",
    "        conversion_trg = type(list(id2idx_trg.keys())[0])\n",
    "    if format == 'matrix':\n",
    "        # Dense\n",
    "        \"\"\"\n",
    "        gt = np.zeros((len(id2idx_src.keys()), len(id2idx_trg.keys())))\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()                \n",
    "                gt[id2idx_src[conversion_src(src)], id2idx_trg[conversion_trg(trg)]] = 1\n",
    "        return gt\n",
    "        \"\"\"\n",
    "        # Sparse\n",
    "        row = []\n",
    "        col = []\n",
    "        val = []\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()\n",
    "                row.append(id2idx_src[conversion_src(src)])\n",
    "                col.append(id2idx_trg[conversion_trg(trg)])\n",
    "                val.append(1)\n",
    "        gt = csr_matrix((val, (row, col)), shape=(len(id2idx_src), len(id2idx_trg)))\n",
    "    else:\n",
    "        gt = {}\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()\n",
    "                # print(src, trg)\n",
    "                if id2idx_src:\n",
    "                    gt[id2idx_src[conversion_src(src)]] = id2idx_trg[conversion_trg(trg)]\n",
    "                else:\n",
    "                    gt[int(src)] = int(trg)\n",
    "    return gt\n",
    "train_dict='graph_data/douban/dictionaries/node,split=0.1.train.dict'\n",
    "train_dict = load_gt(train_dict, source_dataset.id2idx, target_dataset.id2idx, 'dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = target_dataset.G\n",
    "G2_edges = pd.DataFrame(G2.edges()).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7071, 0.7071],\n",
       "        [0.7071, 0.7071]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(torch.tensor([[0.5,0.5],[0.5,0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [1, 2, 3, 4]\n",
    "columns = [5, 6, 7]\n",
    "\n",
    "index_columns = pd.MultiIndex.from_product([index, columns], names=['index', 'columns'])\n",
    "df = pd.DataFrame(index=index_columns).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_array, columns_array = np.meshgrid(index, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array([index_array.ravel(), columns_array.ravel()]).T\n",
    "result[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.zeros((len(index) * len(columns), 3))\n",
    "sim_matrix[:,0] = index_array.ravel()\n",
    "sim_matrix[:,1] = columns_array.ravel()\n",
    "sim_matrix[:,2] = index_array.ravel()+columns_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 9.])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[[5,6],[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes1 = np.array([1, 2, 3])\n",
    "nodes2 = np.array([4, 5, 6])\n",
    "sims = np.array([0.5, 0.8, 0.2])\n",
    "\n",
    "# 将 numpy 数组转换为 torch 张量\n",
    "nodes1_tensor = torch.from_numpy(nodes1)\n",
    "nodes2_tensor = torch.from_numpy(nodes2)\n",
    "sims_tensor = torch.from_numpy(sims)\n",
    "\n",
    "# 创建稀疏张量\n",
    "indices = torch.stack([nodes1_tensor, nodes2_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2', '3'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setA = set('1')\n",
    "setB = set('2')\n",
    "setC = set('3')\n",
    "set.union(setA,setB,setC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 7]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ = {4:5,6:7}\n",
    "list(map(lambda x:dict_[x],np.array([4,6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seed(x):\n",
    "        return x\n",
    "g1 = to_seed(list(df['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def calculate_new(row):\n",
    "    ep = 0.01\n",
    "    #ACNs = len(list(nx.common_neighbors(G,row.name[0],row.name[1]+6))) + ep\n",
    "    print(row['index'])\n",
    "    #print(row.name[0],row.name[1]+6)\n",
    "    ACNs = ep\n",
    "    #Tver = ACNs**2 / (abs(len(setA) - len(setB)) + ep2)\n",
    "    return ACNs**2\n",
    "df['sim'] = df.apply(calculate_new,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th>columns</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sim\n",
       "index columns        \n",
       "1     5        0.0001\n",
       "      6        0.0001\n",
       "      7        0.0001\n",
       "2     5        0.0001\n",
       "      6        0.0001\n",
       "      7        0.0001\n",
       "3     5        0.0001\n",
       "      6        0.0001\n",
       "      7        0.0001\n",
       "4     5        0.0001\n",
       "      6        0.0001\n",
       "      7        0.0001"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "list(nx.common_neighbors(G2,'5','10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "train_dict = 'graph_data/weibo_douban/dictionaries/node,split=0.1.train.dict'\n",
    "\n",
    "part = train_dict.split('=')[1].split('.')\n",
    "\n",
    "split_ratio = part[0]+'.' + part[1]\n",
    "\n",
    "print(split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(source_dataset.G.nodes())  # 获取行数，即字典中最大的键值加1\n",
    "num_cols = len(target_dataset.G.nodes())  # 获取列数，即字典中最大的值加1\n",
    "values = torch.ones(len(train_dict))  # 所有对应位置的值都设置为1\n",
    "indices = torch.tensor([list(train_dict.keys()), list(train_dict.values())], dtype=torch.int64)\n",
    "mat3 = torch.sparse_coo_tensor(indices, values, (num_rows, num_cols))\n",
    "def Sparse_graph(A):\n",
    "    if isinstance(A, np.matrix):\n",
    "        A = torch.FloatTensor(A)\n",
    "    indices = torch.nonzero(A).t()\n",
    "    values = A[indices[0], indices[1]]\n",
    "    A = torch.sparse.FloatTensor(indices, values, A.size())\n",
    "    return A\n",
    "mat1 = Sparse_graph(source_dataset.get_adjacency_matrix())\n",
    "mat2 = Sparse_graph(target_dataset.get_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = target_dataset.get_adjacency_matrix()\n",
    "if isinstance(A, np.matrix):\n",
    "    A = torch.DoubleTensor(A)\n",
    "indices = torch.nonzero(A).t()\n",
    "#values = A[indices[0], indices[1]]\n",
    "degree = torch.tensor(target_dataset.get_nodes_degrees())\n",
    "degree = torch.pow(degree, -1)\n",
    "degree[torch.isinf(degree)] = 0\n",
    "values = A[indices[0], indices[1]] * degree[indices[0]]\n",
    "A = torch.sparse_coo_tensor(indices, values, A.size(),dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 9511, 9512, 9513],\n",
       "                       [   1,   21,   70,  ...,  120,  298,  205]]),\n",
       "       values=tensor([0.0303, 0.0303, 0.0303,  ..., 1.0000, 1.0000, 1.0000]),\n",
       "       size=(9514, 9514), nnz=329586, dtype=torch.float64,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A =A.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2251, -0.0734, -0.1275,  ...,  0.1526,  0.1652,  0.0676],\n",
       "        [-0.0621,  0.1355,  0.1051,  ...,  0.1393, -0.0525, -0.0517],\n",
       "        [ 0.2865, -0.4511, -0.0100,  ..., -0.5873, -0.0099, -0.0343],\n",
       "        ...,\n",
       "        [-0.0157, -1.7015,  0.5399,  ...,  0.4524,  0.0018, -0.5227],\n",
       "        [-0.1711,  0.9633, -0.3868,  ...,  0.4405, -0.2844,  0.2109],\n",
       "        [-0.1595,  1.3394, -0.2785,  ...,  0.5445, -0.2868, -0.9870]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "cur = nn.Embedding(len(target_dataset.G.nodes()),256).weight\n",
    "torch.mm(A,cur.to(torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = Sparse_graph(source_dataset.get_adjacency_matrix(),source_dataset.get_nodes_degrees())\n",
    "mat2 = Sparse_graph(target_dataset.get_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half = torch.cat([mat1, mat3], dim=1)\n",
    "bottom_half = torch.cat([mat2, mat3.t()], dim=1)\n",
    "new_sparse_mat = torch.vstack([top_half, bottom_half])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_source = torch.LongTensor(random.sample(list(list(range(200))),int(200 * 0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got dict_values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb 单元格 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m])}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m b \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m])}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y231sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m c \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([a\u001b[39m.\u001b[39;49mvalues(),b\u001b[39m.\u001b[39;49mvalues()])\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got dict_values"
     ]
    }
   ],
   "source": [
    "if isinstance(A, np.matrix):\n",
    "        A = torch.FloatTensor(A)\n",
    "    indices = torch.nonzero(A).t()\n",
    "    #values = A[indices[0], indices[1]]\n",
    "    degree = torch.sparse.sum(A, dim=1).to_dense()\n",
    "    degree = torch.pow(degree, -1)\n",
    "    degree[torch.isinf(degree)] = 0\n",
    "    values = A[indices[0], indices[1]] * degree[indices[0]]\n",
    "    A = torch.sparse.FloatTensor(indices, values, A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dense features\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "data_dir = 'graph_data/weibo_douban/douban/graphsage/'\n",
    "typea='s'\n",
    "dense_feature_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(data_dir))),'profile_df.pkl')\n",
    "if os.path.isfile(dense_feature_path):\n",
    "    with open(dense_feature_path,'rb') as file:\n",
    "        profile_df = pickle.load(file)\n",
    "    sparse_feature_name = []\n",
    "    dense_feature_name = []\n",
    "    dense_f_list_transforms = {}\n",
    "    profile_columns = ['username','description']\n",
    "    print('loading dense features')\n",
    "    for f in profile_columns:\n",
    "        if f == 'description':\n",
    "            continue\n",
    "    ##如果特征值是一个列表，表示为密集特征，将列表中的元素合并为一个词汇表，并记录词汇表的长度\n",
    "        if type(profile_df[f][0]) == list:\n",
    "            dense_f_list = profile_df[f].values.tolist()\n",
    "            vocab = []\n",
    "            for sub_list in dense_f_list:\n",
    "                for j in sub_list:\n",
    "                    try:\n",
    "                        vocab.append(j)\n",
    "                    except:\n",
    "                        print('empty feature')\n",
    "                        continue\n",
    "            vocab = list(set(vocab))\n",
    "            vocab_len = len(vocab)\n",
    "        \"\"\" dense_f_transform = []\n",
    "        if typea == 's':\n",
    "            dense_f_list = dense_f_list[:9514]\n",
    "        else:\n",
    "            dense_f_list = dense_f_list[9514:]\n",
    "        for t in dense_f_list:\n",
    "            dense_f_idx = torch.zeros(1, vocab_len).long()\n",
    "            for w in t:\n",
    "                idx = vocab.index(w)\n",
    "                dense_f_idx[0, idx] = 1\n",
    "            dense_f_transform.append(dense_f_idx)\n",
    "        dense_f_list_transforms[f] = torch.cat(dense_f_transform, dim=0)\n",
    "        dense_feature_name.append({'feature_name':f}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12693"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_f_transform = []\n",
    "if typea == 's':\n",
    "    dense_f_list = dense_f_list[:9514]\n",
    "else:\n",
    "    dense_f_list = dense_f_list[9514:]\n",
    "for t in dense_f_list:\n",
    "    dense_f_idx = torch.zeros(1, vocab_len).long()\n",
    "    for w in t:\n",
    "        idx = vocab.index(w)\n",
    "        dense_f_idx[0, idx] = 1\n",
    "    dense_f_transform.append(dense_f_idx)\n",
    "dense_f_list_transforms[f] = torch.cat(dense_f_transform, dim=0)\n",
    "dense_feature_name.append({'feature_name':f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['username', 'gender', 'birthday', 'description', 'location'], dtype='object')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dir = 'graph_data/weibo_douban/douban/graphsage/'\n",
    "typea='s'\n",
    "dense_feature_path = os.path.join(os.path.dirname(os.path.dirname(data_dir)),'profile_df.pkl')\n",
    "if os.path.isfile(dense_feature_path):\n",
    "    with open(dense_feature_path,'rb') as file:\n",
    "        profile_df = pickle.load(file)\n",
    "    sparse_feature_name = []\n",
    "    dense_feature_name = []\n",
    "    dense_f_list_transforms = {}\n",
    "    profile_columns = profile_df.columns\n",
    "    print('loading dense features')\n",
    "    for f in profile_columns:\n",
    "    ##如果特征值是一个列表，表示为密集特征，将列表中的元素合并为一个词汇表，并记录词汇表的长度\n",
    "        if type(profile_df[f][0]) == list:\n",
    "            dense_f_list = profile_df[f].values.tolist()\n",
    "            vocab = []\n",
    "            for sub_list in dense_f_list:\n",
    "                for j in sub_list:\n",
    "                    try:\n",
    "                        vocab.append(j)\n",
    "                    except:\n",
    "                        print('empty feature')\n",
    "                        continue\n",
    "            vocab = list(set(vocab))\n",
    "            vocab_len = len(vocab)\n",
    "\n",
    "            #然后，将每个列表转换为一个二进制的张量，其中每个位置表示是否存在对应的词。\n",
    "            dense_f_transform = []\n",
    "            if typea == 's':\n",
    "                dense_f_list = dense_f_list[:len(source_dataset.G.nodes())]\n",
    "            else:\n",
    "                dense_f_list = dense_f_list[len(target_dataset.G.nodes()):]\n",
    "            for t in dense_f_list:\n",
    "                dense_f_idx = torch.zeros(1, vocab_len).long()\n",
    "                for w in t:\n",
    "                    idx = vocab.index(w)\n",
    "                    dense_f_idx[0, idx] = 1\n",
    "                dense_f_transform.append(dense_f_idx)\n",
    "            dense_f_list_transforms[f] = torch.cat(dense_f_transform, dim=0)\n",
    "            dense_feature_name.append({'feature_name':f})\n",
    "            \n",
    "        else:\n",
    "            #如果特征值不是一个列表，表示为稀疏特征，使用LabelEncoder对其进行编码，并记录特征的维度。最后，将物品特征转换为torch.Tensor类型的矩阵。\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(profile_df[f])\n",
    "            profile_df[f] = encoder.transform(profile_df[f])\n",
    "            feature_dim = len(encoder.classes_)\n",
    "            sparse_feature_name.append({'feature_name':f, 'feature_dim':feature_dim})\n",
    "    sparse_feature_matrix = torch.from_numpy(profile_df[[f['feature_name'] for f in sparse_feature_name]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb 单元格 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y213sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m j \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y213sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m index \u001b[39m=\u001b[39m indices[i][j]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y213sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwhile\u001b[39;00m index \u001b[39min\u001b[39;49;00m used_indices:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y213sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     j \u001b[39m=\u001b[39m (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39mvalues\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#Y213sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     index \u001b[39m=\u001b[39m indices[i][j]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "values,indices = torch.sort(score,descending=True)\n",
    "top1_indices = []\n",
    "used_indices = []\n",
    "score_masked = []\n",
    "for i in range(values.shape[0]):\n",
    "    j = 0\n",
    "    index = indices[i][j]\n",
    "    while index in used_indices:\n",
    "        j = (j+1)%values.shape[1]\n",
    "        index = indices[i][j]\n",
    "    top1_indices.append(index)\n",
    "    used_indices.append(index)\n",
    "    score_masked.append(score[i][index])\n",
    "top1_indices = torch.tensor(top1_indices)\n",
    "score_masked = torch.tensor(score_masked)\n",
    "top1_indices,score_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half = torch.sparse.cat([mat1, mat2], dim=1)\n",
    "bottom_half = torch.sparse.cat([torch.zeros_like(padded_mat2), padded_mat2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 假设你有三个稀疏矩阵 mat1, mat2 和 mat3\n",
    "\n",
    "# 将 mat1 和 mat2 分别作为一个大矩阵的左上角和右下角\n",
    "num_rows = max(mat1.size(0), mat2.size(0))\n",
    "num_cols = max(mat1.size(1), mat2.size(1))\n",
    "padded_mat1 = torch.sparse_coo_tensor(mat1._indices(), mat1._values(), (num_rows, num_cols))\n",
    "padded_mat2 = torch.sparse_coo_tensor(mat2._indices(), mat2._values(), (num_rows, num_cols))\n",
    "\n",
    "# 将 mat3 和 mat3 的转置分别作为一个大矩阵的右上角和左下角\n",
    "mat3_transposed = mat3.t()\n",
    "num_rows = max(mat3.size(0), mat3_transposed.size(0))\n",
    "num_cols = max(mat3.size(1), mat3_transposed.size(1))\n",
    "padded_mat3 = torch.sparse_coo_tensor(mat3._indices(), mat3._values(), (num_rows, num_cols))\n",
    "padded_mat3_transposed = torch.sparse_coo_tensor(mat3_transposed._indices(), mat3_transposed._values(), (num_rows, num_cols))\n",
    "\n",
    "# 使用 cat 函数将左上角和右下角组合成一个大矩阵的上半部分\n",
    "top_half = torch.sparse.cat([padded_mat1, torch.zeros_like(padded_mat1)], dim=1)\n",
    "bottom_half = torch.sparse.cat([torch.zeros_like(padded_mat2), padded_mat2], dim=1)\n",
    "\n",
    "# 使用 vstack 函数将上半部分和下半部分组合成一个新的稀疏大矩阵\n",
    "new_sparse_mat = torch.sparse.vstack([top_half, bottom_half])\n",
    "\n",
    "print(new_sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_train(batch):\n",
    "    #可能是s2t或者是t2s\n",
    "    first_node = [item[0] for item in batch]\n",
    "    pos_node = [item[1] for item in batch]\n",
    "    neg_node = [item[2] for item in batch]\n",
    "    print(first_node)\n",
    "\n",
    "    first_node = torch.LongTensor(first_node)\n",
    "    pos_node = torch.LongTensor(pos_node)\n",
    "    neg_node = torch.LongTensor(neg_node)\n",
    "\n",
    "    return [first_node, pos_node, neg_node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "# 假设 edge_attr 是边的权重\n",
    "edge_attr = torch.tensor([0.5, 0.8, 0.2, 0.4])  # 4 条边的权重\n",
    "a = edge_attr.unsqueeze(-1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(torch.Size([32,16]))\n",
    "b = torch.randn(torch.Size([32,16]))\n",
    "c = torch.sum(a*b,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3706, -5.5941, -3.3357,  4.2818,  5.8353, -2.3487, -8.4051, -7.4610,\n",
       "         5.6333,  3.8732,  4.9585, -0.7757, -4.0768, -1.5527,  8.5214, -3.4207,\n",
       "        -3.5349,  5.2682,  6.8391,  5.4776,  0.1327, -0.5032, -6.9506,  0.7837,\n",
       "        -2.1135,  0.9928, -1.1655, 13.3179,  0.2021, -1.9950,  8.7415,  1.1583])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2],\n",
       "        [1, 0, 2, 1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)\n",
    "values = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float)\n",
    "sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([3, 3]))\n",
    "sparse_matrix.coalesce().indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "s2t_train_loader = DataLoader(s2t_train_set, shuffle=True, batch_size=128, collate_fn=my_collate_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(s2t_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" s_embedding = torch.FloatTensor(s_model(source_A_hat,'s').cpu().detach().numpy())\\nt_embedding = torch.FloatTensor(t_model(source_A_hat,'t').cpu().detach().numpy())\\ntorch.save(s_embedding, 's_embedding.pth') \\ntorch.save(t_embedding, 't_embedding.pth')  \""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_A_hat=None\n",
    "s_model = torch.load('s_model.pth')\n",
    "t_model = torch.load('t_model.pth')\n",
    "\"\"\" s_embedding = torch.FloatTensor(s_model(source_A_hat,'s').cpu().detach().numpy())\n",
    "t_embedding = torch.FloatTensor(t_model(source_A_hat,'t').cpu().detach().numpy())\n",
    "torch.save(s_embedding, 's_embedding.pth') \n",
    "torch.save(t_embedding, 't_embedding.pth')  \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_505085/2786302525.py\", line 5, in <module>\n",
      "    s2t_source_outputs,s2t_target_outputs = model.two_forward(None,None,first_node,second_node,'s2t')\n",
      "  File \"/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/algorithms/MGLAlign/model.py\", line 957, in two_forward\n",
      "    #用拉普拉斯效果还行，但还是比不上GAlign的加上refine\n",
      "  File \"/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/algorithms/MGLAlign/model.py\", line 1075, in two_link_predict\n",
      "TypeError: encode() takes 2 positional arguments but 3 were given\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/data/private/interactive63025/bowen/.conda/envs/bowen/lib/python3.8/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "mode = model.eval()\n",
    "s2t_test_set = get_test_set(test_dict,'s2t')\n",
    "s2t_test_loader = DataLoader(s2t_test_set, shuffle=True, batch_size=len(s2t_test_set), collate_fn=collate_two)\n",
    "first_node,second_node = next(iter(s2t_test_loader))\n",
    "s2t_source_outputs,s2t_target_outputs = model.two_forward(None,None,first_node,second_node,'s2t')\n",
    "\"\"\" s2t_source_outputs = model(self.source_A_hat, 's')\n",
    "s2t_target_outputs = model(self.target_A_hat, 't')  \"\"\"\n",
    "full_dict = load_gt(groundtruth, 'dict')\n",
    "s2t_acc, s2t_S = get_acc(s2t_source_outputs, s2t_target_outputs, full_dict, alphas=[0,0,1], just_S=False)\n",
    "print(\"Supervised s2t Acc: {}\".format(s2t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1069, 2044, 2901, 1698, 3725, 2250, 909, 2569, 1784, 2566, 1156, 3340, 2520, 3636, 2953, 871, 811, 583, 2378, 2583, 2032, 1843, 482, 3761, 529, 2687, 1453, 1223, 3556, 740, 251, 1528, 3296, 770, 3125, 1638, 1235, 670, 3297, 559, 1623, 3047, 232, 2298, 598, 2175, 2823, 2, 2430, 330, 1972, 203, 349, 3369, 1264, 3107, 2275, 2407, 2576, 1282, 2343, 1113, 2446, 240, 608, 563, 419, 289, 2932, 3123, 1272, 1415, 2991, 1593, 628, 1017, 3218, 1117, 2246, 1035, 689, 809, 1747, 3013, 3582, 704, 2625, 141, 1656, 3610, 1805, 2891, 3078, 1619, 2139, 1954, 1308, 2269, 2853, 1134, 3237, 3526, 1286, 3510, 1002, 2067, 1328, 3511, 472, 3351, 3533]\n",
      "tensor([1069, 2044, 2901, 1698, 3725, 2250,  909, 2569, 1784, 2566, 1156, 3340,\n",
      "        2520, 3636, 2953,  871,  811,  583, 2378, 2583, 2032, 1843,  482, 3761,\n",
      "         529, 2687, 1453, 1223, 3556,  740,  251, 1528, 3296,  770, 3125, 1638,\n",
      "        1235,  670, 3297,  559, 1623, 3047,  232, 2298,  598, 2175, 2823,    2,\n",
      "        2430,  330, 1972,  203,  349, 3369, 1264, 3107, 2275, 2407, 2576, 1282,\n",
      "        2343, 1113, 2446,  240,  608,  563,  419,  289, 2932, 3123, 1272, 1415,\n",
      "        2991, 1593,  628, 1017, 3218, 1117, 2246, 1035,  689,  809, 1747, 3013,\n",
      "        3582,  704, 2625,  141, 1656, 3610, 1805, 2891, 3078, 1619, 2139, 1954,\n",
      "        1308, 2269, 2853, 1134, 3237, 3526, 1286, 3510, 1002, 2067, 1328, 3511,\n",
      "         472, 3351, 3533])\n"
     ]
    }
   ],
   "source": [
    "for index, (first_node, second_node, neg_node) in enumerate(s2t_train_loader):\n",
    "    print(first_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "attrs = pickle.load(open('../MAUIL-main/data/dblp/attrs','rb'))\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.24387701],\n",
       "       [ 6.79447116],\n",
       "       [ 5.04200278],\n",
       "       ...,\n",
       "       [50.        ],\n",
       "       [50.        ],\n",
       "       [50.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = min(source_dataset.get_nodes_degrees())\n",
    "source_alpha = 1 + 3906/(np.log(source_dataset.get_nodes_degrees()).sum()-np.log(a)*3906)\n",
    "b = np.power(source_dataset.get_nodes_degrees(),-source_alpha)\n",
    "scaled_arr = np.interp(b, (b.min(), b.max()), (5, 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.24387701,  6.79447116,  5.04200278, ..., 50.        ,\n",
       "       50.        , 50.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_arr = np.interp(b, (b.min(), b.max()), (5, 50))\n",
    "scaled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_len = 5\n",
    "switch_prob=0.3\n",
    "num_walks = 5\n",
    "from collections import Counter\n",
    "def add_edge(a, b,adj_dict):\n",
    "    if a in adj_dict:\n",
    "        neighbors = adj_dict[a]\n",
    "    else:\n",
    "        neighbors = set()\n",
    "        adj_dict[a] = neighbors\n",
    "    if b not in neighbors:\n",
    "        neighbors.add(b)\n",
    "    \n",
    "\n",
    "def norm(counter):\n",
    "    s = sum(counter.values())\n",
    "    new_counter = Counter()\n",
    "    for a, count in counter.items():\n",
    "        new_counter[a] = counter[a] / s\n",
    "    return new_counter\n",
    "\n",
    "def compute_2hop_adj(adj):\n",
    "    new_adj = np.zeros_like(adj)\n",
    "    for i in range(len(new_adj)):\n",
    "        new_adj[i] = adj[np.where(adj[i] == 1)[0].astype(int)].sum(axis=0)\n",
    "    new_adj[new_adj>0] = 1\n",
    "    return new_adj\n",
    "\n",
    "def get_min_max_deg(adj, degree):\n",
    "    deg_matrix = adj * degree\n",
    "    max_deg = deg_matrix.max(axis=1)\n",
    "    deg_matrix[deg_matrix == 0] = 1e10\n",
    "    min_deg = deg_matrix.min(axis=1)\n",
    "    return min_deg, max_deg\n",
    "\n",
    "def get_dist_matrix():\n",
    "        \"\"\"\n",
    "        Degree matrix\n",
    "        \"\"\"\n",
    "        nes = len(source_dataset.G.edges())\n",
    "        net = len(target_dataset.G.edges())\n",
    "        nns = len(source_dataset.G.nodes())\n",
    "        nnt = len(target_dataset.G.nodes())\n",
    "        rectified_params_source = np.sqrt((net/nnt) / (nes/nns))\n",
    "        rectified_params_target = 1 / rectified_params_source\n",
    "\n",
    "        source_adj = source_dataset.get_adjacency_matrix()\n",
    "        target_adj = target_dataset.get_adjacency_matrix()\n",
    "        \n",
    "        source_degree = source_adj.sum(axis=1) * rectified_params_source\n",
    "        target_degree = target_adj.sum(axis=1) * rectified_params_target\n",
    "        \n",
    "        source_adj_2hop = compute_2hop_adj(source_adj)\n",
    "        target_adj_2hop = compute_2hop_adj(target_adj)\n",
    "\n",
    "        # we need to define max_deg_neib_source and min_deg_neib_source (target)\n",
    "        min_deg_neib_source, max_deg_neib_source = get_min_max_deg(source_adj, source_degree)\n",
    "        min_deg_neib_target, max_deg_neib_target = get_min_max_deg(target_adj, target_degree)\n",
    "\n",
    "        min_deg_neib_source2, max_deg_neib_source2 = get_min_max_deg(source_adj_2hop, source_degree)\n",
    "        min_deg_neib_target2, max_deg_neib_target2 = get_min_max_deg(target_adj_2hop, target_degree)\n",
    "      \n",
    "        dist = np.zeros((source_adj.shape[0], target_adj.shape[0]))\n",
    "\n",
    "        for i in range(source_adj.shape[0]):\n",
    "            \n",
    "            \"\"\" dist[i] = np.abs(np.log(source_degree[i] + 1) - np.log(target_degree.reshape(1,1118) + 1)) * 2 + dist[i]\n",
    "            dist[i] = np.abs(np.log(min_deg_neib_source[i] + 1) - np.log(min_deg_neib_target.reshape(1,1118) + 1)) + \\\n",
    "                        np.abs(np.log(max_deg_neib_source[i] + 1) - np.log(max_deg_neib_target.reshape(1,1118) + 1)) + dist[i]\n",
    "\n",
    "            dist[i] = np.abs(np.log(min_deg_neib_source2[i] + 1) - np.log(min_deg_neib_target2.reshape(1,1118) + 1)) + \\\n",
    "                        np.abs(np.log(max_deg_neib_source2[i] + 1) - np.log(max_deg_neib_target2.reshape(1,1118) + 1)) + dist[i] \"\"\"\n",
    "\n",
    "            dist[i] = np.abs(np.log(source_degree[i] + 1) - np.log(target_degree.T + 1)) * 2 + dist[i]\n",
    "            dist[i] = np.abs(np.log(min_deg_neib_source[i] + 1) - np.log(min_deg_neib_target.T + 1)) + \\\n",
    "                        np.abs(np.log(max_deg_neib_source[i] + 1) - np.log(max_deg_neib_target.T + 1)) + dist[i]\n",
    "\n",
    "            dist[i] = np.abs(np.log(min_deg_neib_source2[i] + 1) - np.log(min_deg_neib_target2.T + 1)) + \\\n",
    "                        np.abs(np.log(max_deg_neib_source2[i] + 1) - np.log(max_deg_neib_target2.T + 1)) + dist[i]\n",
    "        dist =  np.exp( - 5 * dist)\n",
    "        return dist\n",
    "\n",
    "\n",
    "def dist_to_distdict(dist):\n",
    "    dist_dict = {}\n",
    "    for node in source_dataset.G.nodes():\n",
    "        for target_node in target_dataset.G.nodes():\n",
    "            dist_dict[(source_dataset.id2idx[node], target_dataset.id2idx[target_node]+num_source_nodes)] = dist[source_dataset.id2idx[node], target_dataset.id2idx[target_node]]\n",
    "    return dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 15]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for source in list(source_dataset.G.nodes())[:1]:\n",
    "    hop = 2\n",
    "    hop_num_list = []\n",
    "    frontiers = {source}\n",
    "    travel = [source]\n",
    "    travel_set = {source}\n",
    "    travel_hop = 1\n",
    "    while travel_hop <= hop:\n",
    "        nexts = set()\n",
    "        node_size = 10\n",
    "        for frontier in frontiers:\n",
    "            frontier = int(frontier)\n",
    "            if len(source_dataset.get_nodes_neighbors()[frontier]) > node_size:\n",
    "                node_children = np.random.choice(source_dataset.get_nodes_neighbors()[frontier], node_size, replace=False)\n",
    "            else:\n",
    "                node_children = source_dataset.get_nodes_neighbors()[frontier]\n",
    "            for current in node_children:\n",
    "                if current not in travel_set:\n",
    "                    travel.append(current)\n",
    "                    nexts.add(current)\n",
    "                    travel_set.add(current)\n",
    "        frontiers = nexts\n",
    "        hop_num_list.append(len(nexts))\n",
    "        travel_hop += 1\n",
    "    travel.remove(source)\n",
    "    #feature_embedding = cal_embedding(travel, hop_num_list, p_lambda, deepwalk_embeddings)\n",
    "hop_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb 单元格 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/MGLAlign-master/test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m source_dataset\u001b[39m.\u001b[39mget_nodes_degrees()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "source_dataset.get_nodes_degrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.graph_utils import load_gt\n",
    "num_source_nodes=3906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_gt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#得到的是索引\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_gt\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_data/douban/dictionaries/node,split=0.2.train.dict\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_dataset\u001b[38;5;241m.\u001b[39mid2idx, target_dataset\u001b[38;5;241m.\u001b[39mid2idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\" for key,value in train_dict.items():\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    train_dict[key] = value + num_source_nodes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03minv_train_dict = {v:k for k, v in train_dict.items()} \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_gt' is not defined"
     ]
    }
   ],
   "source": [
    "#得到的是索引\n",
    "train_dict = load_gt(\"graph_data/douban/dictionaries/node,split=0.2.train.dict\", source_dataset.id2idx, target_dataset.id2idx, 'dict')\n",
    "\"\"\" for key,value in train_dict.items():\n",
    "    train_dict[key] = value + num_source_nodes\n",
    "inv_train_dict = {v:k for k, v in train_dict.items()} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_adj_dict = {}\n",
    "target_adj_dict = {}\n",
    "for a, b in source_dataset.get_edges():\n",
    "    add_edge(int(a), int(b),source_adj_dict)\n",
    "    add_edge(int(b), int(a),source_adj_dict)\n",
    "\n",
    "for c, d in target_dataset.get_edges():\n",
    "    add_edge(int(c+num_source_nodes), int(d+num_source_nodes),target_adj_dict)\n",
    "    add_edge(int(d+num_source_nodes), int(c+num_source_nodes),target_adj_dict)\n",
    "\n",
    "source_adj_dict = {a: list(neighbors) for a, neighbors in source_adj_dict.items()}\n",
    "target_adj_dict = {c: list(neighbors) for c, neighbors in target_adj_dict.items()}\n",
    "\n",
    "adj_dict = [source_adj_dict,target_adj_dict]\n",
    "\n",
    "dist_dict = dist_to_distdict(get_dist_matrix())\n",
    "\n",
    "nets = [source_dataset.G, target_dataset.G]\n",
    "\n",
    "# word_counter = Counter()\n",
    "walk_counters = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net_index in range(2):\n",
    "    for node in adj_dict[net_index]:\n",
    "        curr_node = node\n",
    "        curr_net = net_index\n",
    "        #current_path_len = np.random.randint(1, path_len + 1)\n",
    "        for _ in range(num_walks):\n",
    "            for _ in range(path_len):\n",
    "                if node in walk_counters:\n",
    "                    walk_counter = walk_counters[node]\n",
    "                else:\n",
    "                    walk_counter = Counter()\n",
    "                    walk_counters[node] = walk_counter\n",
    "\n",
    "                if np.random.rand() > switch_prob:\n",
    "                    neighbors_deg = [nets[curr_net].degree(str(ele) if curr_net==0 else str(ele-num_source_nodes)) for ele in adj_dict[curr_net][curr_node]]\n",
    "                    next_node = np.random.choice(adj_dict[curr_net][curr_node], p=np.array(neighbors_deg)/np.sum(neighbors_deg))\n",
    "                else:\n",
    "                    # switch net now\n",
    "                    # train_dict的target图索引要加上源图的结点数\n",
    "                    if train_dict.get(curr_node) and curr_net==0:\n",
    "                        next_node = train_dict.get(curr_node)\n",
    "                        curr_net = 1 # switched\n",
    "                    elif inv_train_dict.get(curr_node) and curr_net==1:\n",
    "                        next_node = inv_train_dict.get(curr_node)\n",
    "                        curr_net = 0 # switched\n",
    "                    else:\n",
    "                        cur_to_switch = (curr_net + 1) % 2\n",
    "                        destination_nodes = adj_dict[cur_to_switch].keys()\n",
    "                        if curr_net == 0:\n",
    "                            p = np.array([dist_dict[(curr_node, ele)] for ele in destination_nodes])\n",
    "                        elif curr_net == 1:\n",
    "                            p = np.array([dist_dict[(ele, curr_node)] for ele in destination_nodes])\n",
    "                        p /= p.sum()\n",
    "                        next_node = np.random.choice(list(destination_nodes), p=p)\n",
    "                        curr_net = cur_to_switch # switched\n",
    "                walk_counter[next_node] += 1\n",
    "                curr_node = next_node\n",
    "\n",
    "normed_walk_counters = {a: norm(walk_counter) for a, walk_counter in walk_counters.items()}\n",
    "\n",
    "prob_sums = Counter()\n",
    "\n",
    "for a, normed_walk_counter in normed_walk_counters.items():\n",
    "    for b, prob in normed_walk_counter.items():\n",
    "        prob_sums[b] += prob\n",
    "\n",
    "ppmis = {}\n",
    "\n",
    "for a, normed_walk_counter in normed_walk_counters.items():\n",
    "    for b, prob in normed_walk_counter.items():\n",
    "        ppmi = np.log(prob / prob_sums[b] * len(prob_sums) / path_len)\n",
    "        ppmis[(a, b)] = ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random walk process\")\n",
    "pairs = []\n",
    "nets = [self.Gs, self.Gt]\n",
    "suffices = [0, len(self.Gs.nodes())]\n",
    "id2idices = [self.id2idxs, self.id2idxt]\n",
    "\n",
    "cur = 0\n",
    "\n",
    "for net_index in range(2):\n",
    "    for node in nets[net_index].nodes():\n",
    "        for _ in range(self.num_walks):\n",
    "            if nets[net_index].degree(node) == 0:\n",
    "                continue\n",
    "            curr_node = node\n",
    "            cur = net_index\n",
    "            for _ in range(self.walk_len):\n",
    "                if np.random.rand() > self.switch_prob:\n",
    "                    # choose by degree distribution\n",
    "                    \n",
    "                    neighbors = nets[cur].neighbors(curr_node)\n",
    "                    list_neighbors= []\n",
    "                    for neighbor in neighbors:\n",
    "                        list_neighbors.append(neighbor)\n",
    "                    neighbors_deg = [nets[cur].degree(ele) for ele in list_neighbors]\n",
    "                    next_node = np.random.choice(list_neighbors, p=np.array(neighbors_deg)/np.sum(neighbors_deg))\n",
    "                    curr_node = next_node\n",
    "                else:\n",
    "                    # switch net now\n",
    "                    if self.test_dict.get(curr_node) and cur==0:\n",
    "                        next_node = self.test_dict.get(curr_node)\n",
    "                        cur = 1 # switched\n",
    "                        curr_node = next_node\n",
    "                    elif self.inv_test_dict.get(curr_node) and cur==1:\n",
    "                        next_node = self.inv_test_dict.get(curr_node)\n",
    "                        cur = 0 # switched\n",
    "                        curr_node = next_node\n",
    "                    else:\n",
    "                        cur_to_switch = (cur + 1) % 2\n",
    "                        destination_nodes = nets[cur_to_switch].nodes()\n",
    "                        if cur == 0:\n",
    "                            p = np.array([self.dist_dict[(curr_node, ele)] for ele in destination_nodes])\n",
    "                        elif cur == 1:\n",
    "                            p = np.array([self.dist_dict[(ele, curr_node)] for ele in destination_nodes])\n",
    "                        p /= p.sum()\n",
    "                        next_node = np.random.choice(destination_nodes, p=p)\n",
    "                        cur = cur_to_switch # switched\n",
    "                        curr_node = next_node\n",
    "                if curr_node != node or cur != net_index:\n",
    "                    pairs.append([id2idices[net_index][node] + suffices[net_index], id2idices[cur][curr_node] + suffices[cur]])\n",
    "    cur = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Laplacian_graph(A):\n",
    "    for i in range(len(A)):\n",
    "        A[i, i] = 1\n",
    "    A = torch.FloatTensor(A)\n",
    "    D_ = torch.diag(torch.sum(A, 0)**(-0.5))\n",
    "    A_hat = torch.matmul(torch.matmul(D_,A),D_)\n",
    "    A_hat = A_hat.float()\n",
    "    indices = torch.nonzero(A_hat).t()\n",
    "    values = A_hat[indices[0], indices[1]]\n",
    "    A_hat = torch.sparse.FloatTensor(indices, values, A_hat.size())\n",
    "    return A_hat, coo_matrix(A.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_feats = source_dataset.features\n",
    "target_feats = target_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix2sparse(matrix):\n",
    "    tensor = torch.Tensor(matrix)\n",
    "    indices = torch.nonzero(tensor).t()\n",
    "    values = tensor[indices[0], indices[1]]\n",
    "    coo_tensor = torch.sparse.FloatTensor(indices, values, tensor.size())\n",
    "    return coo_tensor\n",
    "source_adj = matrix2sparse(source_dataset.get_adjacency_matrix())\n",
    "target_adj = matrix2sparse(target_dataset.get_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gt(path, id2idx_src=None, id2idx_trg=None, format='matrix'):    \n",
    "    if id2idx_src:\n",
    "        conversion_src = type(list(id2idx_src.keys())[0])\n",
    "        conversion_trg = type(list(id2idx_trg.keys())[0])\n",
    "    if format == 'matrix':\n",
    "        # Dense\n",
    "        \"\"\"\n",
    "        gt = np.zeros((len(id2idx_src.keys()), len(id2idx_trg.keys())))\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()                \n",
    "                gt[id2idx_src[conversion_src(src)], id2idx_trg[conversion_trg(trg)]] = 1\n",
    "        return gt\n",
    "        \"\"\"\n",
    "        # Sparse\n",
    "        row = []\n",
    "        col = []\n",
    "        val = []\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()\n",
    "                row.append(id2idx_src[conversion_src(src)])\n",
    "                col.append(id2idx_trg[conversion_trg(trg)])\n",
    "                val.append(1)\n",
    "        gt = csr_matrix((val, (row, col)), shape=(len(id2idx_src), len(id2idx_trg)))\n",
    "    else:\n",
    "        gt = {}\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()\n",
    "                # print(src, trg)\n",
    "                if id2idx_src:\n",
    "                    gt[id2idx_src[conversion_src(src)]] = id2idx_trg[conversion_trg(trg)]\n",
    "                else:\n",
    "                    gt[str(src)] = str(trg)\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = load_gt(\"graph_data/douban/dictionaries/groundtruth\", source_dataset.id2idx, target_dataset.id2idx, 'dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 1116, 1117, 1117],\n",
       "                       [  45,  575,  834,  ..., 3902,  723, 1373]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(1118, 3906), nnz=7848, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_indices = torch.LongTensor(list(groundtruth.keys()))\n",
    "target_indices = torch.LongTensor(list(groundtruth.values()))\n",
    "source_adj.index_select(0,source_indices)\n",
    "target_adj.index_select(0,target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_adj = torch.sparse.mm(source_adj.index_select(0,source_indices).t(),target_adj.index_select(0,target_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 1.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 4\n",
    "col=4\n",
    "rate=0.25\n",
    "zeros_num = int(row* rate)\n",
    "new_array = np.ones((row,col))\n",
    "for i in range(row):\n",
    "    indices = random.sample(range(col),zeros_num)\n",
    "    new_array[i,indices] = 0\n",
    "new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'groundtruth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb 单元格 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pair \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mlist\u001b[39m(groundtruth\u001b[39m.\u001b[39mkeys()), \u001b[39mlist\u001b[39m(groundtruth\u001b[39m.\u001b[39mvalues())))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m pair\n",
      "\u001b[0;31mNameError\u001b[0m: name 'groundtruth' is not defined"
     ]
    }
   ],
   "source": [
    "neg_list = []\n",
    "for key,value in train_dict.items():\n",
    "    neg_node = random.choice(list(range(num)))\n",
    "    if type == 's2t':\n",
    "        while neg_node == value:\n",
    "            neg_node = random.choice(list(range(num)))\n",
    "    else:\n",
    "        while neg_node == key:\n",
    "            neg_node = random.choice(list(range(num)))\n",
    "    neg_list.append(neg_node)\n",
    "if type == 's2t':\n",
    "    pair = list(zip(list(train_dict.keys()), list(train_dict.values()),neg_list))\n",
    "else:\n",
    "    pair = list(zip(list(train_dict.values()), list(train_dict.keys()),neg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = []\n",
    "for key,values in groundtruth.items():\n",
    "    neg_node = random.choice(list(range(1118)))\n",
    "    while neg_node == values:\n",
    "        neg_node = random.choice(list(range(1118)))\n",
    "    neg_list.append(neg_node)\n",
    "neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_A_hat, _ = Laplacian_graph(source_dataset.get_adjacency_matrix())\n",
    "source_A_mixup_hat,_ = Laplacian_graph(source_mixup_dataset.get_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,value in enumerate(source_dataset.get_nodes_neighbors()):\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for element in source_dataset.get_nodes_neighbors()[0]:\n",
    "    print(source_dataset.id2idx[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keyiterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb 单元格 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb#Y205sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlist\u001b[39m(source_dataset\u001b[39m.\u001b[39mG\u001b[39m.\u001b[39mneighbors(\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb#Y205sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m source_dataset\u001b[39m.\u001b[39mid2idx[source_dataset\u001b[39m.\u001b[39;49mG\u001b[39m.\u001b[39;49mneighbors(\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keyiterator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "list(source_dataset.G.neighbors('0'))\n",
    "source_dataset.id2idx[source_dataset.G.neighbors('0')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = list(range(len(source_feats)))\n",
    "target_ids = list(range(len(target_feats)))\n",
    "np.random.shuffle(source_ids)\n",
    "np.random.shuffle(target_ids)\n",
    "shuffled_source_feats = source_feats[source_ids]\n",
    "shuffled_target_feats = target_feats[target_ids]\n",
    "row = source_A_hat.coalesce().indices()[0]\n",
    "col = source_A_hat.coalesce().indices()[1]\n",
    "new_indices = torch.stack([source_ids[row], source_ids[col]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 501,  501,  501,  ...,  646, 1558, 1558],\n",
       "        [ 501, 3141, 3429,  ...,  646, 3685, 1558]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = source_A_hat.coalesce().indices()[0]\n",
    "col = source_A_hat.coalesce().indices()[1]\n",
    "new_indices = torch.stack([torch.tensor([source_ids[element.item()] for element in row]), torch.tensor([source_ids[element.item()] for element in col])], dim=0)\n",
    "new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 501,  501,  501,  ...,  646, 1558, 1558])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([source_ids[element.item()] for element in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,  ..., 3904, 3905, 3905])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_A_hat.coalesce().indices()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_source_A_hat = source_A_hat.coalesce().clone()\n",
    "source_row = source_A_hat.coalesce().indices()[0]\n",
    "shuffled_source_A_hat.coalesce().indices()[0] = torch.tensor([source_ids[element.item()] for element in source_row])\n",
    "source_col = source_A_hat.coalesce().indices()[1]\n",
    "shuffled_source_A_hat.coalesce().indices()[1] = torch.tensor([source_ids[element.item()] for element in source_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1731490/5846151.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(mean,ignore_index=True).rename(index={len(results): 'mean_values'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MAP</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_5</th>\n",
       "      <th>Precision_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4284</td>\n",
       "      <td>0.5381</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.7701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4320</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.6780</td>\n",
       "      <td>0.7737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4284</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.4267</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>0.7818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4258</td>\n",
       "      <td>0.5345</td>\n",
       "      <td>0.9899</td>\n",
       "      <td>0.4186</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4374</td>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.6807</td>\n",
       "      <td>0.7728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_values</th>\n",
       "      <td>0.4304</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.7725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy     MAP     AUC  Precision_1  Precision_5  Precision_10\n",
       "0              0.4284  0.5381  0.9904       0.4240       0.6807        0.7701\n",
       "1              0.4320  0.5415  0.9906       0.4293       0.6780        0.7737\n",
       "2              0.4284  0.5423  0.9904       0.4267       0.6771        0.7818\n",
       "3              0.4258  0.5345  0.9899       0.4186       0.6789        0.7639\n",
       "4              0.4374  0.5432  0.9905       0.4293       0.6807        0.7728\n",
       "mean_values    0.4304  0.5399  0.9904       0.4256       0.6791        0.7725"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv('0_experiment_results.csv')\n",
    "mean = results.mean()\n",
    "results = results.append(mean,ignore_index=True).rename(index={len(results): 'mean_values'})\n",
    "results.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1731490/1415775847.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(mean,ignore_index=True).rename(index={len(results): 'mean_values'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MAP</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_5</th>\n",
       "      <th>Precision_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.4249</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.7692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4267</td>\n",
       "      <td>0.5323</td>\n",
       "      <td>0.9899</td>\n",
       "      <td>0.4275</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>0.7701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4338</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.6762</td>\n",
       "      <td>0.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4374</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.4365</td>\n",
       "      <td>0.6762</td>\n",
       "      <td>0.7665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4347</td>\n",
       "      <td>0.5460</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>0.6789</td>\n",
       "      <td>0.7683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_values</th>\n",
       "      <td>0.4331</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.6719</td>\n",
       "      <td>0.7708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy     MAP     AUC  Precision_1  Precision_5  Precision_10\n",
       "0              0.4329  0.5408  0.9908       0.4249       0.6673        0.7692\n",
       "1              0.4267  0.5323  0.9899       0.4275       0.6610        0.7701\n",
       "2              0.4338  0.5370  0.9904       0.4302       0.6762        0.7800\n",
       "3              0.4374  0.5457  0.9907       0.4365       0.6762        0.7665\n",
       "4              0.4347  0.5460  0.9901       0.4320       0.6789        0.7683\n",
       "mean_values    0.4331  0.5403  0.9904       0.4302       0.6719        0.7708"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('1_experiment_results.csv')\n",
    "mean = results.mean()\n",
    "results = results.append(mean,ignore_index=True).rename(index={len(results): 'mean_values'})\n",
    "results.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_degrees(neibor_dict):\n",
    "        degrees = {}\n",
    "        for key,value in neibor_dict.items():\n",
    "            degrees[key] = len(value)\n",
    "        return degrees\n",
    "degrees1 = gen_degrees(neibor_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_file = \"graph_data/douban/online/shuffle/graphsage/G.json\"\n",
    "G_data = json.load(open(nodes_file))\n",
    "G = json_graph.node_link_graph(G_data)\n",
    "\n",
    "all_nodes = np.array(G.nodes())\n",
    "all_edges = G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file = 'graph_data/douban/online/shuffle/graphsage/feats.npy'\n",
    "all_features = None\n",
    "if os.path.isfile(feature_file):\n",
    "    all_features = np.load(feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.random.beta(4.0, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_node = {}\n",
    "for key,value in neibor_dict1.items():\n",
    "    max_node = list(value)[0]\n",
    "    for index,node in enumerate(value):\n",
    "        if degrees1[node] > degrees1[max_node]:\n",
    "            max_node = node\n",
    "    mixup_node[key] = max_node\n",
    "mixup_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Population must be a sequence or set.  For dicts, use list(d).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb 单元格 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B167.251:600-bowen/opt/data/private/interactive63025/bowen/Documents/GAlign-master/test.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m random\u001b[39m.\u001b[39;49msample(mixup_node,\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/bowen/lib/python3.8/random.py:359\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    357\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(population)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(population, _Sequence):\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPopulation must be a sequence or set.  For dicts, use list(d).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    360\u001b[0m randbelow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow\n\u001b[1;32m    361\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(population)\n",
      "\u001b[0;31mTypeError\u001b[0m: Population must be a sequence or set.  For dicts, use list(d)."
     ]
    }
   ],
   "source": [
    "random.sample(mixup_node,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_features = np.zeros(all_features.shape)\n",
    "mixup_neighbor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in mixup_node.items():\n",
    "    mixup_features[key] = all_features[key]*lam + (1-lam)*all_features[value]\n",
    "    #mixup_degrees[key] = round(degrees1[key]*lam + (1-lam)*degrees1[value])\n",
    "    #TODO:对于边这种离散的，到底怎么mixup，对度数mixup以后该如何选择保留和增加哪些边，如果通过训练自适应应该怎么弄，先试试随机\n",
    "    #有两种思路，一种是在原本的图基础上对满足要求的节点mixup节点属性和度数即边，这样节点总数不变；另一种是生成新的节点，原本的节点要保留不变，节点总数变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_dict = copy.deepcopy(neibor_dict1)\n",
    "for key,value in mixup_node.items():\n",
    "    mixup_neighbor[key] = set(random.sample(neighbor_dict[key]-{value}, round(degrees1[key]*lam)-1)) | set(random.sample(neighbor_dict[value]-{key}, round(degrees1[value]*(1-lam)))) | {value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "1\n",
      "132\n",
      "196\n",
      "969\n",
      "973\n",
      "666\n",
      "1243\n",
      "862\n",
      "95\n",
      "1406\n",
      "421\n",
      "1261\n",
      "367\n",
      "369\n",
      "2229\n",
      "182\n",
      "760\n",
      "2041\n",
      "382\n"
     ]
    }
   ],
   "source": [
    "for key,value in mixup_neighbor.items():\n",
    "    if key==0:\n",
    "        for index,neighbor in enumerate(value):\n",
    "            print(neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 434, 925, 969, 1243, 2041, 2065, 2229}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = neighbor_dict[0]-{666}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 434, 666, 925, 969, 1243, 2041, 2065, 2229}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DIFF_S = torch.load('DIFFAlign_smi_normalize.pt').detach().cpu()\n",
    "DL_S = torch.load('DeepLink_smi.pt').detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([211,  46, 211,  ..., 211, 211, 211])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indices_DIFF = torch.argmax(DIFF_S,dim=1)\n",
    "max_indices_DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([568, 550, 299,  ..., 940, 226, 848])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indices_DEEP = torch.argmax(DL_S,dim=1)\n",
    "max_indices_DEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4748659.5000)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(DIFF_S-DL_S).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3906, 1118])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL_S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.6187,  0.7752,  0.9923,  ...,  0.4665, -0.1100,  0.0605]),\n",
       " tensor([0.9858, 0.7646, 1.0632,  ..., 1.4942, 0.7939, 1.3230]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIFF_S[0],DL_S[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embedding_DIFF = torch.load('DIFFAlign_data.pt')[0].detach().cpu()\n",
    "target_embedding_DIFF = torch.load('DIFFAlign_data.pt')[1].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embedding_DEEP = torch.load('DeepLink_data.pt')[0].detach().cpu()\n",
    "target_embedding_DEEP = torch.load('DeepLink_data.pt')[1].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3400.8481)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(source_embedding_DIFF-source_embedding_DEEP).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, MAP, top5, top10, AUC,top1 = get_statistics(S, groundtruth, use_greedy_match=False, get_all_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset.get_edges()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_link_data(G, attrs=None):\n",
    "    \"\"\"Returns data in node-link format that is suitable for JSON serialization\n",
    "    and use in Javascript documents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : NetworkX graph\n",
    "\n",
    "    attrs : dict\n",
    "        A dictionary that contains five keys 'source', 'target', 'name',\n",
    "        'key' and 'link'.  The corresponding values provide the attribute\n",
    "        names for storing NetworkX-internal graph data.  The values should\n",
    "        be unique.  Default value::\n",
    "\n",
    "            dict(source='source', target='target', name='id',\n",
    "                 key='key', link='links')\n",
    "\n",
    "        If some user-defined graph data use these attribute names as data keys,\n",
    "        they may be silently dropped.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "       A dictionary with node-link formatted data.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NetworkXError\n",
    "        If values in attrs are not unique.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from networkx.readwrite import json_graph\n",
    "    >>> G = nx.Graph([(\"A\", \"B\")])\n",
    "    >>> data1 = json_graph.node_link_data(G)\n",
    "    >>> H = nx.gn_graph(2)\n",
    "    >>> data2 = json_graph.node_link_data(\n",
    "    ...     H, {\"link\": \"edges\", \"source\": \"from\", \"target\": \"to\"}\n",
    "    ... )\n",
    "\n",
    "    To serialize with json\n",
    "\n",
    "    >>> import json\n",
    "    >>> s1 = json.dumps(data1)\n",
    "    >>> s2 = json.dumps(\n",
    "    ...     data2, default={\"link\": \"edges\", \"source\": \"from\", \"target\": \"to\"}\n",
    "    ... )\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Graph, node, and link attributes are stored in this format.  Note that\n",
    "    attribute keys will be converted to strings in order to comply with JSON.\n",
    "\n",
    "    Attribute 'key' is only used for multigraphs.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    node_link_graph, adjacency_data, tree_data\n",
    "    \"\"\"\n",
    "    multigraph = G.is_multigraph()\n",
    "    # Allow 'attrs' to keep default values.\n",
    "    if attrs is None:\n",
    "        attrs = _attrs\n",
    "    else:\n",
    "        attrs.update({k: v for (k, v) in _attrs.items() if k not in attrs})\n",
    "    name = attrs[\"name\"]\n",
    "    source = attrs[\"source\"]\n",
    "    target = attrs[\"target\"]\n",
    "    links = attrs[\"link\"]\n",
    "    # Allow 'key' to be omitted from attrs if the graph is not a multigraph.\n",
    "    key = None if not multigraph else attrs[\"key\"]\n",
    "    if len({source, target, key}) < 3:\n",
    "        raise nx.NetworkXError(\"Attribute names are not unique.\")\n",
    "    data = {\n",
    "        \"directed\": G.is_directed(),\n",
    "        \"multigraph\": multigraph,\n",
    "        \"graph\": G.graph,\n",
    "        \"nodes\": [dict(chain(G.nodes[n].items(), [(name, n)])) for n in G],\n",
    "    }\n",
    "    if multigraph:\n",
    "        data[links] = [\n",
    "            dict(chain(d.items(), [(source, u), (target, v), (key, k)]))\n",
    "            for u, v, k, d in G.edges(keys=True, data=True)\n",
    "        ]\n",
    "    else:\n",
    "        data[links] = [\n",
    "            dict(chain(d.items(), [(source, u), (target, v)]))\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x7fcc158e7f70>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_src = type(source_dataset.G.nodes['0']) \n",
    "conversion_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = source_dataset.get_adjacency_matrix()\n",
    "A2 = target_dataset.get_adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf, nan\n",
    "n1 = A1.shape[0]\n",
    "n2 = A2.shape[0]\n",
    "d1 = 1 / A1.sum(axis=1)\n",
    "d2 = 1 / A2.sum(axis=1)\n",
    "d1[d1 == inf] = 0\n",
    "d2[d2 == inf] = 0\n",
    "d1 = d1.reshape(-1,1)*np.ones(d1.T.shape)\n",
    "d2 = d2.reshape(-1,1)*np.ones(d2.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf, nan\n",
    "n1 = A1.shape[0]\n",
    "n2 = A2.shape[0]\n",
    "d1 = 1 / A1.sum(axis=1)\n",
    "d2 = 1 / A2.sum(axis=1)\n",
    "d1[d1 == inf] = 0\n",
    "d2[d2 == inf] = 0\n",
    "d1 = np.array(d1.reshape(-1,1))\n",
    "d2 = np.array(d2.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: nt, Created on: Mon Nov 19 14:36:11 2018',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'H': array([[0.28942624, 0.        , 0.2532979 , ..., 0.        , 0.        ,\n",
       "         0.19280219],\n",
       "        [0.        , 0.        , 0.        , ..., 0.07902424, 0.        ,\n",
       "         0.        ],\n",
       "        [0.18542083, 0.        , 0.38791765, ..., 0.14129021, 0.        ,\n",
       "         0.13306451],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.1961202 , 0.        ,\n",
       "         0.22020819],\n",
       "        [0.        , 0.        , 0.        , ..., 0.15542971, 0.        ,\n",
       "         0.15794968],\n",
       "        [0.31150195, 0.61817088, 0.23167164, ..., 0.        , 0.        ,\n",
       "         0.19244519]])}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "dict_H = loadmat('graph_data/douban/H.mat')\n",
    "dict_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = source_dataset.G\n",
    "G2 = target_dataset.G\n",
    "res1 = json_graph.node_link_data(G1)\n",
    "res2 = json_graph.node_link_data(G2)\n",
    "id2idx1 = source_dataset.id2idx\n",
    "id2idx2 = target_dataset.id2idx\n",
    "\n",
    "new_nodes_idxs1 = np.arange(len(G1.nodes()))\n",
    "new_nodes_idxs2 = np.arange(len(G1.nodes()), len(G1.nodes()) + len(G2.nodes()))\n",
    "\n",
    "new_nodes = []\n",
    "for idx, node in enumerate(res1[\"nodes\"]):\n",
    "    original_index = id2idx1[node[\"id\"]]\n",
    "    node[\"id\"] = str(original_index)\n",
    "    new_nodes.append(node)\n",
    "for idx, node in enumerate(res2[\"nodes\"]):\n",
    "    original_index = id2idx2[node[\"id\"]]\n",
    "    node[\"id\"] = str(int(original_index) + len(G1.nodes()))\n",
    "    new_nodes.append(node)\n",
    "\n",
    "new_id2idx = {}\n",
    "for node in new_nodes:\n",
    "    new_id2idx[node[\"id\"]] = int(node[\"id\"])\n",
    "\n",
    "new_links = []\n",
    "for link in res1[\"links\"]:\n",
    "    new_source_index = str(id2idx1[link[\"source\"]])\n",
    "    new_target_index = str(id2idx1[link[\"target\"]])\n",
    "    new_links.append({\n",
    "        'source': new_source_index,\n",
    "        'target': new_target_index\n",
    "    })\n",
    "for link in res2[\"links\"]:\n",
    "    new_source_index = str(int(id2idx2[link[\"source\"]]) + len(G1.nodes()))\n",
    "    new_target_index = str(int(id2idx2[link[\"target\"]]) + len(G1.nodes()))\n",
    "    if int(new_source_index) > 11724 or int(new_target_index) >11724:\n",
    "        print('yes')\n",
    "    new_links.append({\n",
    "        'source': new_source_index,\n",
    "        'target': new_target_index\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = None\n",
    "features1 = source_dataset.features\n",
    "features2 = target_dataset.features\n",
    "\n",
    "if features1 is not None and features2 is not None:\n",
    "    if features1.shape[1] != features2.shape[1]:\n",
    "        print(\"Can not create new features due to different features shape.\")\n",
    "    new_features = np.zeros((features1.shape[0] + features2.shape[0], features1.shape[1]))\n",
    "    for i, feat in enumerate(features1):\n",
    "        new_features[i] = feat\n",
    "    for i, feat in enumerate(features2):\n",
    "        new_features[i+len(G1.nodes())] = feat\n",
    "\n",
    "new_res = json_graph.node_link_data(G1)\n",
    "new_res[\"nodes\"] = new_nodes\n",
    "new_res[\"links\"] = new_links\n",
    "\n",
    "G = json_graph.node_link_graph(new_res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView(('2', '25', '166', '265', '274', '347', '458', '481', '509', '537', '672', '771', '917', '930', '1137', '1199', '1207', '1282', '1338', '1464', '1545', '1737', '1760', '1809', '2978', '3301', '3363', '3479', '3490', '3542', '3616', '3742', '3833', '3844', '3912', '4285', '4386', '4441', '4564', '4596', '4741', '4884', '4930', '5040', '5140', '5591', '5631', '5671', '5696', '6156', '6225', '6286', '6378', '3', '290', '863', '1586', '2041', '2043', '2326', '2592', '2600', '2948', '2962', '3074', '3085', '3099', '3116', '3121', '3418', '3480', '3622', '3658', '3681', '4189', '4307', '4316', '4385', '4464', '4843', '4854', '5009', '5461', '5704', '5758', '6395', '4', '11', '293', '311', '380', '395', '443', '475', '496', '668', '737', '800', '843', '918', '992', '1058', '1068', '1076', '1139', '1281', '1371', '1711', '1716', '2124', '2258', '2262', '2431', '2439', '2596', '2800', '2807', '2944', '2968', '3080', '3288', '3440', '3530', '3587', '3694', '3919', '4245', '4408', '4474', '4498', '4571', '4679', '4891', '4936', '4953', '4959', '5021', '5054', '5176', '5227', '5426', '5520', '5528', '5558', '5897', '6207', '6238', '5', '168', '185', '255', '334', '356', '358', '497', '527', '561', '618', '642', '650', '661', '679', '722', '931', '980', '1011', '1161', '1206', '1216', '1225', '1267', '1273', '1295', '1563', '1593', '1624', '1707', '1710', '1733', '1761', '1769', '1799', '1965', '2085', '2561', '2618', '2685', '2728', '2730', '2754', '2766', '2827', '2865', '2886', '2939', '2983', '3156', '3199', '3209', '3231', '3401', '3445', '3569', '3582', '3613', '3649', '3650', '3656', '3665', '3764', '4175', '4247', '4304', '4338', '4398', '4465', '4500', '4505', '4718', '4751', '4804', '4806', '4970', '5020', '5076', '5077', '5094', '5125', '5157', '5200', '5264', '5266', '5439', '5504', '5533', '5543', '5563', '5619', '5636', '5799', '5840', '6034', '6071', '6177', '6188', '6301', '6320', '6350', '6381', '6464', '6', '250', '266', '402', '416', '554', '563', '669', '693', '859', '865', '883', '1045', '1230', '1320', '1372', '1419', '1659', '1730', '2125', '2204', '3034', '3275', '3538', '3726', '3782', '4242', '4246', '4397', '4447', '4516', '4551', '4600', '4612', '4757', '4814', '4869', '4931', '4938', '4983', '5062', '5065', '5071', '5102', '5338', '5349', '5351', '5359', '5366', '5381', '5449', '5472', '5593', '5611', '5616', '5655', '5783', '5939', '6281', '6312', '6343', '6380', '6405', '6471', '7', '384', '420', '468', '613', '640', '727', '864', '874', '907', '1286', '1467', '1582', '1709', '1817', '1823', '1921', '2229', '2869', '3205', '3295', '3404', '3450', '3553', '3690', '3727', '4011', '4229', '4243', '4311', '4426', '4730', '5380', '5425', '5447', '5574', '5677', '5862', '8', '441', '790', '828', '1098', '2320', '2329', '2493', '4922', '5339', '5764', '9', '117', '400', '452', '459', '494', '515', '754', '860', '1347', '1491', '1597', '2567', '2764', '2904', '3088', '3101', '3109', '3176', '3251', '3435', '3792', '4348', '4394', '5126', '5223', '5622', '10', '456', '526', '796', '1167', '1405', '1739', '2758', '4272', '4536', '4996', '5400', '5446', '5592', '87', '317', '377', '544', '731', '805', '849', '1134', '1294', '1380', '1455', '1976', '2410', '2476', '2521', '2682', '3002', '3087', '3299', '3460', '3556', '3652', '3689', '3758', '3975', '4232', '4241', '4400', '4562', '4638', '4750', '4811', '4872', '4877', '5207', '5231', '5249', '5443', '5477', '5579', '5726', '5809', '5870', '6252', '6284', '6384', '12', '22', '27', '85', '89', '96', '102', '223', '357', '359', '367', '410', '442', '504', '598', '635', '821', '942', '1060', '1211', '1251', '1254', '1397', '1421', '1424', '1443', '1463', '1513', '1531', '1554', '1653', '1999', '2082', '2557', '2852', '3142', '3357', '3469', '3526', '3527', '3565', '3706', '3857', '4006', '4214', '4256', '4352', '4370', '4401', '4482', '4630', '4665', '4771', '4780', '4789', '5099', '5179', '5204', '5322', '5498', '5557', '5672', '5965', '6031', '6180', '6199', '6204', '6237', '6264', '6326', '13', '43', '224', '257', '300', '323', '373', '466', '480', '534', '619', '744', '817', '825', '832', '836', '850', '966', '986', '1461', '1474', '1568', '1585', '1720', '2097', '2216', '2343', '2643', '2749', '3206', '3328', '3374', '3849', '4111', '4178', '4253', '4313', '4416', '4548', '4561', '4569', '4734', '4967', '5107', '5178', '5243', '5302', '5530', '5795', '5984', '6002', '6128', '6134', '6214', '6247', '6260', '6357', '6366', '6406', '14', '60', '62', '217', '270', '332', '585', '1241', '1345', '1429', '1476', '1532', '1612', '1633', '1828', '1840', '1906', '1948', '2068', '2123', '2580', '2628', '2669', '2787', '2845', '2900', '3016', '3122', '3193', '3264', '3386', '3463', '3594', '3634', '3761', '3796', '4197', '4449', '4460', '4631', '4634', '4661', '4722', '4798', '4824', '4998', '5061', '5165', '5743', '6169', '6427', '6457', '6469', '15', '49', '55', '112', '134', '152', '351', '437', '491', '493', '519', '572', '615', '740', '755', '892', '1023', '1033', '1141', '1142', '1145', '1157', '1162', '1198', '1203', '1326', '1352', '1395', '1408', '1449', '1481', '1490', '1617', '1627', '1682', '1749', '1789', '1912', '2560', '2603', '2762', '2831', '2874', '2992', '3015', '3039', '3062', '3097', '3203', '3310', '3355', '3378', '3507', '3513', '3522', '3541', '3620', '3629', '3640', '3679', '3702', '3938', '4071', '4200', '4239', '4340', '4365', '4456', '4497', '4522', '4580', '4647', '4654', '4660', '4820', '4907', '4911', '4913', '4942', '4958', '4974', '5055', '5060', '5136', '5154', '5189', '5225', '5244', '5257', '5274', '5287', '5304', '5342', '5350', '5430', '5464', '5496', '5523', '5539', '5623', '5642', '5676', '5678', '5914', '5970', '6052', '6233', '6242', '6321', '6345', '6373', '6386', '6456', '16', '159', '200', '308', '310', '463', '511', '522', '593', '632', '645', '647', '696', '732', '750', '915', '925', '987', '1024', '1025', '1054', '1155', '1212', '1235', '1244', '1304', '1313', '1325', '1415', '1437', '1498', '1507', '1686', '1721', '1796', '1816', '2268', '2504', '2547', '2604', '2612', '2689', '2735', '2887', '3280', '3462', '3591', '3642', '4640', '4846', '4972', '5471', '5596', '6147', '6185', '6372', '6459', '17', '532', '599', '1101', '1318', '1938', '2435', '2473', '2483', '2526', '2857', '4475', '6055', '18', '264', '284', '327', '525', '684', '846', '1237', '1262', '1316', '1317', '1398', '1441', '1497', '1558', '1631', '1675', '1781', '1841', '1940', '2035', '2575', '2843', '2974', '3057', '3106', '3152', '3324', '3372', '3390', '3554', '3600', '3667', '3808', '3841', '3989', '4260', '4281', '4438', '4485', '4823', '4828', '5082', '5309', '5482', '5699', '5986', '6159', '6229', '19', '127', '218', '299', '390', '418', '595', '676', '912', '1087', '1123', '1181', '1192', '1447', '1502', '1519', '1660', '1777', '2607', '2788', '2818', '3001', '3215', '3549', '3593', '4308', '4476', '4484', '4512', '4626', '4975', '5004', '5166', '5576', '5585', '5763', '5948', '6117', '6226', '6275', '6444', '6454', '20', '297', '389', '708', '720', '809', '975', '1064', '1085', '1292', '1342', '2734', '3075', '3115', '3208', '3249', '3415', '3461', '4932', '4960', '5002', '5466', '5575', '6090', '6396', '6462', '140', '229', '329', '342', '350', '393', '716', '1051', '1074', '1081', '1537', '1550', '1862', '2902', '3114', '3166', '3369', '4191', '4205', '4410', '4541', '4659', '5095', '5499', '5877', '6339', '23', '344', '398', '1147', '1517', '1592', '2958', '3493', '3755', '3795', '4219', '4463', '4477', '5341', '5613', '24', '35', '40', '97', '155', '226', '353', '381', '429', '523', '590', '603', '678', '1022', '1035', '1057', '1416', '1471', '1478', '1526', '1647', '1663', '1743', '2325', '2335', '2366', '2385', '2415', '2419', '2429', '2485', '2510', '3464', '3639', '4275', '4329', '4787', '5112', '5837', '6308', '6419', '29', '73', '108', '399', '921', '1370', '1496', '1944', '2653', '2895', '2989', '3042', '3185', '3186', '3567', '4293', '5265', '5280', '5659', '26', '460', '734', '842', '928', '978', '1149', '1355', '1403', '1452', '1529', '1642', '4195', '4435', '5037', '5089', '5323', '5330', '5962', '6298', '45', '65', '107', '141', '202', '209', '221', '232', '247', '345', '484', '499', '667', '675', '721', '780', '782', '794', '797', '896', '971', '988', '1094', '1278', '1279', '1472', '1620', '1628', '1632', '1719', '1762', '2277', '2342', '2359', '2381', '2405', '2451', '2466', '2467', '2601', '2646', '2757', '2774', '2859', '2972', '3172', '3189', '3437', '3443', '3451', '3551', '3709', '3769', '3839', '3840', '3932', '3990', '4105', '4270', '4277', '4302', '4333', '4381', '4409', '4432', '4506', '4547', '4574', '4577', '4668', '4688', '4733', '4743', '4747', '4763', '4805', '4836', '5044', '5068', '5083', '5085', '5087', '5175', '5239', '5245', '5253', '5356', '5515', '5550', '5566', '5648', '5707', '5861', '6122', '6253', '6329', '6333', '6401', '28', '1095', '1288', '1742', '1788', '1953', '2246', '2344', '2349', '2402', '2425', '2528', '2541', '2544', '2546', '3017', '3344', '4206', '5360', '354', '467', '714', '968', '1114', '1358', '1547', '1766', '1793', '1884', '2039', '2321', '2610', '2794', '2871', '2888', '2947', '3033', '3091', '3623', '3774', '4220', '4655', '4731', '4915', '4925', '5216', '5369', '6015', '6153', '6162', '6323', '6441', '30', '149', '649', '706', '1050', '1097', '1240', '1269', '1516', '2044', '2823', '3201', '3477', '3801', '4354', '4830', '5753', '6192', '31', '80', '115', '234', '551', '559', '571', '739', '769', '784', '1004', '1093', '1119', '1197', '1215', '1428', '1552', '1697', '1712', '1936', '2512', '2708', '2883', '3144', '3399', '3573', '3722', '3732', '3791', '3848', '4190', '4265', '4446', '4514', '4524', '4794', '5019', '5090', '5194', '5246', '5362', '5420', '5435', '5481', '5487', '5823', '6069', '32', '120', '145', '170', '188', '245', '476', '553', '959', '1070', '1186', '1233', '2721', '3670', '4240', '4388', '4433', '4434', '4437', '4472', '4494', '4739', '4758', '4821', '4842', '4850', '5298', '5406', '5643', '5785', '5985', '6067', '6293', '6335', '6443', '33', '54', '164', '254', '674', '944', '1311', '1354', '1392', '1434', '1560', '1606', '1674', '2008', '2032', '2748', '2819', '2930', '2931', '3141', '3331', '3671', '3830', '4268', '4362', '4375', '4590', '4762', '5081', '5214', '5276', '5340', '5358', '5469', '5475', '5521', '5560', '5681', '5705', '5846', '6027', '6109', '6193', '6324', '6360', '6420', '6477', '34', '153', '387', '611', '1382', '2145', '3311', '5975', '483', '943', '1010', '1383', '1644', '2332', '2411', '3032', '3407', '3762', '3797', '3847', '5373', '5541', '5792', '6039', '6413', '36', '67', '183', '304', '352', '569', '768', '916', '1007', '1089', '1287', '1433', '1572', '2119', '2316', '2337', '2458', '2880', '3974', '5064', '37', '135', '370', '555', '579', '685', '757', '1032', '1075', '1146', '1148', '1205', '1256', '1321', '1477', '1556', '1676', '1800', '1898', '2099', '2129', '2172', '2516', '2961', '3048', '3614', '4335', '4417', '4466', '4490', '4496', '4521', '4627', '4955', '4999', '5312', '5470', '5673', '5888', '5998', '6140', '6258', '6328', '6363', '38', '130', '307', '439', '462', '818', '878', '947', '1330', '1420', '1648', '1725', '1779', '1787', '2001', '2089', '2563', '2659', '2879', '2915', '3439', '3662', '3822', '4644', '4670', '4817', '4899', '4944', '4989', '5012', '5119', '5211', '5226', '5318', '5392', '5590', '5594', '5690', '6382', '6436', '39', '92', '104', '190', '193', '238', '303', '333', '346', '634', '715', '795', '852', '899', '1135', '1219', '1436', '1459', '1479', '1487', '1512', '1591', '1702', '1792', '3397', '3802', '4222', '4964', '4978', '5618', '5662', '6451', '1573', '41', '253', '405', '502', '845', '869', '873', '876', '923', '1067', '1231', '1299', '1364', '1387', '1569', '1911', '2067', '2673', '2813', '2950', '3218', '3373', '3417', '3532', '3701', '4296', '4310', '4357', '4361', '4382', '4549', '4857', '4886', '4908', '5544', '5589', '5597', '5747', '5821', '6221', '6307', '6349', '42', '301', '837', '1028', '1484', '1565', '1982', '2828', '3031', '3151', '3414', '3427', '3453', '3478', '3685', '4249', '4452', '4611', '4774', '4832', '5038', '5205', '5701', '5727', '5790', '6196', '292', '2729', '3179', '3821', '4141', '4486', '5567', '6235', '6270', '44', '148', '952', '1140', '1439', '2269', '2270', '2291', '2318', '2360', '2423', '2424', '2475', '2677', '2695', '2798', '2935', '2956', '3125', '3293', '3315', '3598', '3668', '4230', '5296', '5335', '5605', '5666', '5685', '417', '724', '967', '1319', '1399', '1621', '1649', '2619', '2630', '2691', '2717', '2741', '2746', '3040', '3168', '3216', '3254', '3624', '4198', '4422', '4934', '4984', '5007', '5017', '5063', '5133', '5162', '5193', '5254', '5442', '5509', '5625', '5905', '6123', '6269', '46', '199', '252', '319', '335', '375', '438', '495', '545', '575', '957', '970', '1034', '1110', '1116', '1246', '1280', '1381', '1401', '1470', '1510', '1511', '1587', '1638', '1690', '2333', '2334', '2382', '2394', '2868', '3641', '3827', '4364', '4711', '4724', '4941', '5215', '5479', '47', '169', '222', '241', '316', '556', '898', '976', '1021', '1079', '1175', '1204', '1248', '1296', '1495', '1515', '1626', '1780', '1831', '1955', '2306', '3126', '3306', '3391', '3790', '4418', '4487', '5384', '5598', '5635', '6013', '6098', '6110', '6262', '6461', '48', '646', '838', '1151', '1263', '1297', '1610', '1662', '1671', '2283', '2339', '2468', '2654', '2701', '2810', '2878', '3018', '3291', '3517', '4414', '4517', '4591', '4933', '5300', '3384', '3458', '3655', '50', '57', '204', '760', '775', '839', '884', '985', '1136', '1208', '1268', '1353', '1361', '1546', '1872', '2670', '2725', '2744', '2782', '2885', '3162', '3196', '3298', '3307', '3319', '3370', '3635', '3711', '4379', '5024', '5405', '5514', '5713', '6155', '6170', '6316', '51', '6309', '52', '171', '385', '428', '695', '700', '791', '851', '890', '938', '977', '1351', '1356', '1533', '1574', '1615', '1773', '1926', '2699', '2720', '2872', '3110', '3237', '3449', '3707', '3807', '3842', '4188', '4274', '4332', '4405', '4424', '4719', '4755', '5057', '5123', '5233', '5604', '5874', '6061', '6438', '53', '558', '612', '623', '2014', '2222', '3136', '3776', '4300', '4613', '4728', '4940', '5608', '5654', '5683', '5983', '6040', '6091', '482', '855', '888', '891', '894', '1276', '1314', '1988', '2977', '3302', '3434', '4623', '4912', '5106', '5486', '5628', '6341', '260', '267', '498', '1124', '1308', '1700', '1802', '2190', '2196', '2571', '2688', '2718', '2882', '2905', '2985', '3008', '3053', '3194', '3221', '3274', '3300', '3314', '3316', '3352', '3467', '3561', '3566', '3626', '3739', '3888', '4031', '4202', '4294', '4402', '4419', '4602', '4648', '4785', '4845', '4880', '4903', '4921', '5059', '5156', '5177', '5293', '5478', '5617', '6195', '6421', '6472', '56', '181', '455', '471', '489', '580', '616', '655', '698', '743', '758', '833', '956', '993', '1245', '1261', '1272', '1432', '1440', '1445', '1450', '1505', '2143', '2503', '2597', '2674', '2740', '2844', '2856', '2976', '3084', '3256', '3342', '3494', '3804', '3906', '4044', '4233', '4319', '4450', '4470', '4583', '4662', '4703', '4714', '4779', '4791', '4906', '5016', '5025', '5079', '5086', '5101', '5212', '5219', '5261', '5311', '5320', '5416', '5612', '5656', '6154', '6246', '6248', '6283', '6299', '6337', '6375', '6407', '6423', '6435', '324', '658', '904', '1886', '3729', '6458', '58', '78', '262', '604', '914', '1144', '1180', '1348', '1394', '1523', '1668', '1994', '2095', '2940', '3502', '3543', '4252', '4305', '4312', '4532', '4683', '4691', '4773', '4865', '5720', '6352', '59', '576', '1153', '1645', '1998', '2177', '2179', '2231', '3113', '5117', '5630', '6358', '144', '597', '820', '1412', '2054', '2562', '2569', '2623', '2821', '2834', '2897', '2957', '3036', '3104', '3171', '3263', '3368', '3506', '3691', '4099', '4286', '6231', '6474', '61', '312', '409', '844', '877', '1234', '1583', '1713', '1981', '2644', '3559', '4192', '4882', '4894', '4939', '5572', '5697', '5717', '5756', '5760', '6135', '6481', '778', '823', '1920', '1929', '2000', '2795', '3066', '3131', '3491', '3696', '3793', '3832', '5073', '5765', '6077', '6342', '63', '95', '121', '207', '779', '856', '910', '1298', '1661', '1839', '1852', '1901', '2555', '2617', '2710', '2777', '3377', '3723', '3756', '3952', '4707', '4784', '4799', '4962', '5294', '5394', '5536', '6139', '6338', '6412', '6424', '64', '1427', '1589', '1619', '1838', '2154', '2263', '2275', '2478', '2480', '2488', '2532', '2534', '2558', '2714', '3005', '3191', '3746', '3824', '5737', '6179', '6310', '6355', '6478', '432', '1062', '1407', '1714', '4224', '4586', '4849', '6439', '66', '143', '236', '291', '543', '712', '1465', '1731', '1931', '2122', '2161', '2296', '2301', '2389', '2648', '2696', '2891', '2896', '3035', '3213', '3546', '3621', '3845', '4390', '4503', '4585', '4738', '4840', '5305', '5571', '5600', '6029', '6303', '6440', '6445', '295', '363', '412', '949', '1030', '1222', '2279', '2310', '2406', '2548', '2553', '2614', '4946', '6222', '68', '478', '665', '742', '1013', '1103', '1168', '1404', '1444', '1540', '2042', '2050', '2278', '2586', '2589', '2743', '2791', '2796', '2987', '3265', '3501', '3688', '4320', '4413', '4511', '4658', '4705', '4737', '4796', '4829', '4838', '4861', '4895', '5131', '5141', '5159', '5206', '5282', '5329', '5433', '5497', '5532', '5535', '5626', '6083', '6223', '6250', '6272', '6336', '69', '90', '269', '368', '440', '449', '490', '764', '1315', '1376', '1442', '1639', '1693', '1850', '2234', '2556', '2566', '2573', '2841', '3060', '3129', '3130', '3354', '3365', '3455', '3607', '3654', '3660', '4317', '4367', '4544', '4553', '4735', '4736', '4852', '4870', '5182', '5192', '5198', '5248', '5301', '5453', '5503', '5917', '5957', '6318', '6332', '70', '4324', '71', '114', '690', '691', '811', '1209', '1336', '1989', '2144', '2866', '2946', '3140', '3188', '3292', '3487', '3733', '4187', '5048', '5256', '5429', '5500', '6171', '6359', '6361', '110', '180', '378', '488', '538', '546', '592', '733', '772', '1037', '1468', '5118', '6003', '74', '94', '445', '548', '730', '765', '982', '1435', '1630', '1826', '1922', '2019', '2116', '2779', '2862', '2922', '2999', '3107', '3214', '3531', '3676', '4087', '4280', '4605', '4813', '5034', '5313', '5344', '5609', '5689', '5811', '6304', '6431', '75', '500', '617', '903', '1012', '1344', '3006', '3159', '3483', '3536', '3664', '3778', '4680', '5224', '5436', '5556', '5895', '6356', '6374', '76', '162', '507', '1049', '1557', '1618', '2034', '2184', '2901', '2955', '4186', '4573', '5353', '5428', '5526', '5859', '6064', '6133', '6219', '77', '799', '1250', '1584', '2629', '2778', '3000', '3198', '3426', '3512', '3572', '4184', '4713', '259', '282', '1106', '1274', '1458', '1549', '2378', '2702', '2752', '3022', '3069', '4035', '4291', '4396', '4706', '4883', '5035', '5096', '5184', '5586', '79', '167', '583', '701', '961', '1289', '1541', '1670', '2909', '4887', '5386', '5751', '6268', '91', '240', '286', '362', '547', '686', '788', '798', '991', '1538', '1566', '1685', '1825', '2112', '2486', '2517', '2585', '2768', '2808', '2873', '2906', '2986', '3181', '3202', '3349', '3444', '3602', '3633', '3666', '3811', '4421', '4563', '4727', '4997', '5331', '6152', '6215', '6230', '6465', '81', '423', '470', '717', '911', '1483', '1570', '1941', '2299', '2336', '2357', '2396', '2421', '2422', '2450', '82', '415', '586', '749', '1019', '1257', '1264', '1605', '1767', '1768', '2078', '2771', '3028', '3410', '3636', '4236', '4299', '4371', '4412', '4538', '4610', '4671', '4848', '4856', '4909', '4943', '5047', '5267', '6023', '6240', '6302', '6325', '83', '198', '322', '376', '425', '469', '633', '831', '847', '875', '1003', '1052', '1053', '1129', '1243', '1357', '1508', '1609', '1614', '1801', '1855', '1986', '2075', '2579', '2964', '3063', '3474', '3485', '3592', '3628', '3715', '4415', '4458', '4592', '4674', '4675', '4833', '4837', '4839', '4867', '4956', '5001', '5104', '5480', '5581', '5645', '5977', '6060', '6211', '6344', '84', '1882', '4535', '4694', '6024', '6172', '6202', '637', '1365', '1611', '1727', '1820', '3083', '3160', '3268', '6012', '1100', '1881', '2515', '3105', '3161', '3197', '3262', '3317', '3498', '3651', '4559', '4684', '5127', '5183', '5716', '88', '192', '812', '965', '1031', '1065', '1113', '1118', '1122', '1128', '1169', '1223', '1283', '1469', '1499', '1578', '2167', '2584', '2672', '3150', '3210', '3217', '3223', '3238', '3528', '3803', '3838', '4420', '5733', '5938', '6194', '306', '699', '770', '1238', '1259', '1927', '2153', '2692', '2727', '2731', '2877', '3047', '3411', '3438', '3508', '3618', '3638', '4342', '4664', '5250', '5459', '5924', '736', '886', '940', '960', '1083', '1384', '1422', '1665', '1932', '2639', '2860', '3056', '3856', '4165', '4292', '4443', '4673', '4914', '5069', '5493', '5537', '5552', '5649', '5768', '5927', '6078', '131', '325', '364', '430', '600', '629', '681', '707', '893', '1090', '1177', '1196', '1312', '1349', '1527', '1643', '1655', '2197', '2265', '2363', '2481', '2952', '3154', '4204', '4208', '4235', '4278', '4491', '4851', '4896', '5546', '5639', '6209', '6245', '6480', '330', '348', '1385', '1446', '1669', '4666', '93', '125', '161', '191', '206', '408', '450', '524', '989', '1009', '1104', '1521', '1641', '1692', '2038', '4701', '5143', '220', '662', '1154', '1217', '1518', '1530', '1722', '2681', '2914', '3242', '4287', '4373', '4641', '182', '287', '607', '631', '729', '1014', '1026', '1293', '1378', '1561', '1602', '1613', '1893', '3267', '3459', '3669', '4196', '4325', '4330', '4716', '4749', '4764', '4765', '4781', '4822', '4868', '5155', '5172', '5397', '5410', '5969', '6106', '6136', '6158', '6294', '122', '278', '338', '709', '946', '1363', '1791', '1954', '2249', '2461', '2993', '3134', '4218', '4228', '4231', '4374', '5042', '5152', '5247', '5260', '5275', '5456', '5570', '5588', '5754', '6403', '6449', '383', '648', '789', '6103', '98', '315', '596', '692', '866', '1201', '1220', '1266', '1328', '1756', '2971', '3011', '3102', '3119', '3388', '3396', '3584', '3826', '4032', '4112', '4276', '4295', '4444', '4642', '4704', '4723', '4834', '4954', '4977', '5014', '5091', '5180', '5241', '5292', '5444', '5485', '5555', '5561', '5875', '5899', '6285', '6319', '99', '178', '189', '518', '996', '1332', '1652', '1703', '1729', '1782', '1887', '1951', '1959', '1990', '2588', '2850', '2870', '2984', '3305', '3423', '4259', '4552', '4625', '4685', '4729', '4772', '4807', '4864', '4981', '5328', '5454', '5996', '6175', '6409', '100', '272', '406', '578', '786', '1063', '1377', '1438', '1599', '1608', '1657', '2829', '3348', '3814', '4636', '4988', '5120', '5236', '5258', '5283', '6385', '6417', '101', '606', '861', '1214', '2825', '3095', '3266', '3698', '5074', '248', '339', '461', '1718', '2195', '3949', '4080', '4284', '4391', '4489', '4572', '4700', '4717', '4897', '5032', '5303', '5332', '5682', '103', '711', '807', '954', '1194', '2716', '2945', '3335', '3476', '3750', '3751', '3823', '4519', '5018', '5474', '142', '953', '969', '1291', '1466', '2033', '2918', '2934', '3013', '3045', '3540', '3819', '4211', '4363', '4740', '5088', '5424', '5548', '6182', '6187', '105', '111', '542', '584', '591', '627', '688', '1132', '1525', '1967', '2130', '2272', '2632', '3050', '3178', '3749', '4448', '4681', '4905', '5164', '6054', '6391', '285', '1673', '2187', '3987', '4089', '4589', '4593', '4782', '5052', '5167', '1302', '1322', '2806', '3405', '5399', '5718', '6058', '116', '567', '3081', '3117', '3673', '3773', '3986', '5793', '5829', '275', '625', '783', '1005', '1339', '1500', '745', '2201', '3149', '3234', '3345', '3346', '3441', '3680', '3996', '4624', '4643', '4695', '5529', '5750', '6289', '113', '2189', '4366', '4948', '5686', '5830', '5920', '2205', '3287', '3735', '3794', '5363', '1189', '2570', '3500', '213', '279', '508', '570', '573', '762', '1130', '1188', '1410', '1503', '1634', '1687', '2128', '3590', '3631', '3805', '3876', '4116', '4515', '4555', '4669', '4759', '4969', '6288', '6353', '6364', '298', '939', '2609', '2913', '3192', '3643', '4083', '118', '211', '219', '283', '474', '654', '984', '1163', '1275', '1306', '1409', '1672', '1736', '2449', '2932', '3051', '3583', '3678', '4478', '4543', '4698', '4800', '5046', '5056', '5901', '5915', '5931', '6000', '6263', '119', '464', '683', '2031', '5976', '239', '677', '753', '1048', '1184', '1369', '1625', '1842', '2598', '2736', '2772', '2923', '2925', '3123', '3222', '3233', '3252', '3334', '3361', '3820', '4860', '4879', '5383', '5502', '5703', '6257', '6296', '6317', '6362', '6416', '6429', '197', '210', '465', '824', '830', '1038', '1091', '1485', '1696', '2839', '2990', '3224', '3286', '4343', '4558', '4653', '4835', '5010', '5290', '5559', '6144', '212', '638', '867', '1174', '1228', '1797', '1813', '1894', '1913', '2131', '2186', '3043', '3137', '3322', '3339', '3347', '3816', '4529', '4725', '4803', '5208', '5222', '5370', '5427', '5606', '5693', '5935', '6470', '123', '1448', '2255', '2259', '2340', '2379', '2388', '2414', '2445', '2522', '2538', '2602', '2811', '3170', '3836', '5284', '5731', '5744', '5940', '124', '5379', '126', '827', '1362', '1389', '2120', '2683', '2826', '2919', '3065', '3276', '3560', '3661', '3674', '3753', '3759', '3779', '3976', '4210', '4523', '4581', '4863', '5121', '6482', '451', '2175', '4127', '4901', '5039', '128', '179', '273', '520', '962', '1086', '1126', '1170', '1580', '1738', '1814', '1869', '2559', '2732', '2924', '2981', '3329', '3413', '3595', '4429', '5272', '6327', '6473', '129', '392', '487', '560', '748', '826', '973', '990', '1069', '1190', '1567', '2792', '3645', '4314', '4488', '4546', '4597', '4599', '4715', '4768', '4876', '4885', '4995', '5026', '5896', '5908', '6137', '6387', '972', '1305', '1388', '3021', '3325', '4289', '4368', '4399', '4881', '5490', '5542', '5627', '6291', '6354', '132', '1150', '133', '924', '1600', '1689', '2288', '3236', '3431', '3529', '3854', '5786', '6146', '6168', '6340', '435', '854', '1073', '1285', '2183', '2224', '3563', '4057', '4504', '4510', '4618', '4797', '5050', '5108', '5516', '5657', '6208', '776', '897', '3041', '3246', '3432', '4467', '5299', '136', '3393', '5540', '137', '3473', '3851', '5092', '5722', '5850', '139', '184', '230', '309', '374', '609', '630', '792', '858', '932', '1066', '1112', '2245', '2289', '2292', '2293', '2297', '2302', '2317', '2345', '2356', '2440', '2457', '2465', '2477', '2489', '4225', '4234', '5889', '3244', '5440', '6448', '626', '2225', '371', '521', '983', '1598', '1805', '2109', '2620', '3165', '3468', '3514', '4126', '4215', '4904', '5058', '5084', '5111', '5422', '5802', '6261', '2009', '3843', '834', '881', '1055', '1877', '2362', '2525', '2933', '6442', '187', '2755', '3059', '3442', '3588', '4288', '4345', '4359', '4614', '4732', '5235', '6102', '6273', '6305', '146', '361', '506', '862', '937', '997', '1096', '1182', '1604', '1654', '1786', '2817', '3817', '4179', '4565', '4601', '5027', '5168', '5217', '5269', '5295', '6212', '147', '999', '1152', '1164', '1255', '1482', '1577', '1705', '1867', '2416', '2738', '2840', '3270', '3318', '3321', '3367', '3475', '3562', '3684', '3798', '4792', '5075', '5221', '5262', '5412', '5414', '5602', '5894', '6455', '1324', '1350', '1708', '2211', '2412', '5644', '448', '2842', '3212', '3297', '3394', '3610', '5288', '5376', '5825', '5881', '150', '1195', '1635', '3147', '3211', '4377', '5880', '5887', '151', '424', '503', '541', '589', '751', '1084', '1172', '1346', '1493', '1539', '2012', '2024', '2070', '2176', '3547', '4425', '4428', '4632', '4692', '5158', '5582', '6210', '6295', '557', '945', '3734', '4328', '4360', '4927', '5078', '5664', '369', '540', '4902', '5067', '154', '756', '974', '1156', '1236', '2448', '4492', '4518', '4990', '5232', '258', '1042', '1133', '1559', '1601', '1821', '2260', '2501', '2608', '2995', '3260', '3720', '4351', '4550', '4966', '5409', '5419', '6124', '6267', '156', '372', '1138', '1176', '1664', '1890', '2572', '2642', '2833', '2926', '2937', '3020', '3139', '3157', '3400', '3558', '3683', '3714', '4183', '6425', '157', '427', '447', '1044', '1303', '1390', '1534', '1845', '2063', '2622', '2793', '2849', '3004', '3058', '3112', '3489', '3946', '4290', '4298', '4315', '4350', '4423', '4533', '4991', '5134', '5242', '5321', '5325', '5377', '5387', '5650', '5651', '5709', '5826', '5949', '6306', '913', '2747', '5128', '5438', '160', '2203', '2498', '2864', '3767', '3775', '5142', '5278', '5361', '5868', '1227', '1333', '4539', '6347', '6426', '163', '472', '752', '761', '3789', '4826', '4827', '5385', '165', '1183', '1864', '2287', '2427', '2456', '2494', '6430', '2760', '276', '294', '1271', '1391', '1406', '1528', '2003', '3364', '5909', '6021', '6037', '6334', '433', '804', '1374', '1834', '2093', '2180', '2574', '4620', '6432', '1160', '2135', '2667', '2928', '5658', '5777', '741', '905', '1704', '1715', '2149', '2484', '2949', '3323', '3697', '4339', '4355', '4431', '4582', '5137', '5527', '5882', '6042', '6079', '6236', '172', '704', '829', '1579', '1684', '1803', '4635', '5632', '6266', '689', '173', '1249', '3155', '174', '4406', '4708', '4808', '5784', '175', '340', '1080', '177', '900', '1681', '2254', '2294', '2469', '2502', '4056', '4898', '5610', '1340', '5306', '5773', '6044', '477', '1329', '1489', '2724', '3089', '3544', '3813', '5457', '6174', '407', '1945', '3024', '3605', '3828', '4992', '5186', '5488', '5695', '5807', '6234', '6314', '208', '2208', '3533', '4103', '4742', '5110', '5352', '5465', '6346', '2508', '3184', '3812', '4131', '5463', '5667', '6017', '196', '2552', '6095', '1524', '1952', '4508', '6190', '186', '636', '1088', '1386', '1555', '1595', '2861', '3341', '3763', '4263', '4825', '5053', '5252', '6271', '531', '1451', '5210', '5577', '6379', '231', '281', '663', '935', '1071', '1105', '2954', '4258', '4594', '4639', '4924', '5181', '5188', '444', '492', '2899', '3785', '4250', '4980', '4994', '5827', '5934', '6018', '6411', '3029', '3108', '4407', '4513', '4971', '5524', '5788', '5843', '6001', '620', '1242', '1284', '394', '5317', '5326', '6279', '194', '4021', '195', '1027', '1848', '2712', '3183', '2391', '2420', '5723', '1571', '3340', '3402', '4193', '6028', '366', '713', '719', '1117', '1193', '1691', '1746', '1903', '2988', '3030', '3093', '3128', '3281', '3351', '4213', '6450', '355', '1260', '1301', '2155', '2218', '2462', '2549', '710', '747', '815', '1099', '1131', '2855', '3784', '4949', '4968', '5893', '201', '328', '403', '535', '872', '1265', '1590', '2025', '2624', '3078', '3180', '3398', '3406', '3408', '3482', '3520', '3564', '3619', '3699', '4264', '4349', '5286', '5423', '6428', '421', '2765', '3240', '3278', '3436', '3521', '3596', '4383', '5599', '6290', '203', '2193', '4341', '4393', '4453', '6239', '6276', '4889', '4918', '5187', '5767', '205', '243', '816', '819', '857', '2107', '2233', '2605', '3273', '3389', '4237', '4501', '4769', '4920', '4982', '5114', '5230', '5458', '5473', '5646', '5923', '6166', '6370', '2096', '2686', '6138', '379', '919', '1522', '4788', '4858', '3086', '3632', '2273', '2305', '2307', '2482', '2647', '2722', '3523', '4327', '4483', '5103', '4185', '1977', '2079', '2745', '2690', '3777', '4194', '6265', '214', '1006', '6292', '215', '622', '1425', '1843', '1892', '3428', '4207', '4261', '4588', '4622', '5132', '5595', '216', '5857', '5937', '436', '1745', '2098', '2181', '2822', '3611', '3980', '4153', '4356', '4923', '5854', '6087', '6100', '6119', '242', '2751', '5817', '5973', '6205', '3606', '3752', '5755', '431', '5169', '5569', '5698', '1431', '1677', '2236', '2960', '3175', '513', '2058', '2188', '2638', '2662', '2898', '3470', '5008', '5268', '5603', '6216', '1107', '1393', '3195', '4947', '5505', '6151', '6330', '225', '4273', '5407', '1239', '1576', '1874', '2460', '227', '1202', '2700', '2832', '4440', '4604', '4677', '228', '1759', '1829', '2256', '3589', '4690', '1159', '2781', '3094', '4844', '5391', '5781', '1121', '2264', '2308', '2331', '2386', '2495', '2403', '2581', '2635', '2660', '2959', '2997', '3412', '3420', '3585', '4323', '4326', '4469', '5992', '6114', '6397', '3358', '3429', '4526', '4853', '5398', '5564', '233', '235', '1728', '5853', '1000', '1835', '2625', '3073', '3967', '4473', '4554', '5070', '6297', '723', '840', '1411', '1488', '1925', '2162', '2322', '2324', '2364', '2444', '2533', '2846', '1402', '2616', '2770', '2803', '5348', '5849', '5956', '941', '1200', '1486', '1734', '2248', '2284', '2327', '2991', '3098', '3362', '4058', '4979', '5148', '5652', '4693', '6392', '256', '767', '2426', '3503', '3760', '4022', '4657', '4890', '4957', '5234', '5402', '5858', '6452', '244', '1413', '2613', '5987', '671', '2853', '4761', '246', '656', '933', '2241', '5547', '5800', '288', '2706', '3972', '4468', '4531', '4965', '5494', '659', '251', '644', '703', '1307', '1744', '2138', '2599', '2786', '2830', '2980', '3279', '3343', '4309', '4540', '6056', '6393', '1747', '2047', '4919', '5607', '1221', '1520', '1726', '3138', '3169', '3380', '3392', '3023', '3230', '3608', '5403', '5735', '6070', '6251', '802', '1158', '1763', '1975', '6399', '950', '1334', '5036', '271', '773', '1041', '1918', '5413', '5679', '261', '1748', '1775', '1856', '1873', '3284', '5116', '5432', '5583', '5812', '5971', '1543', '6256', '413', '660', '2675', '3326', '4916', '5135', '5417', '6243', '6383', '4182', '2893', '3359', '4568', '3046', '3741', '5568', '6224', '6415', '530', '621', '2348', '2665', '2982', '4070', '4221', '4499', '4875', '5565', '5668', '5688', '5780', '5866', '2261', '2776', '3731', '2797', '5251', '1379', '1637', '2253', '2311', '2509', '4047', '4961', '670', '2970', '3337', '3601', '4481', '6315', '277', '1247', '1331', '1764', '1847', '2951', '3574', '3737', '909', '3303', '3825', '5151', '5285', '280', '5006', '2045', '4528', '4855', '3003', '5834', '6161', '1870', '926', '1979', '4645', '5105', '5963', '6348', '2459', '2464', '3940', '2753', '3703', '4667', '5365', '1426', '3330', '3333', '5629', '6049', '6217', '289', '528', '536', '2711', '3693', '4238', '4819', '5319', '5863', '5982', '1933', '2763', '3914', '4097', '5739', '870', '5310', '787', '6460', '2407', '2463', '2519', '296', '349', '1143', '1457', '2889', '3625', '4461', '4646', '5003', '6141', '6322', '562', '726', '1373', '1623', '1902', '4993', '2202', '2761', '3728', '4689', '6126', '302', '401', '587', '2036', '2640', '3055', '6446', '305', '4818', '5030', '5441', '624', '2103', '2168', '6173', '2006', '1535', '1544', '1551', '1683', '2938', '3079', '3597', '3745', '5913', '6191', '3837', '2351', '3072', '6143', '313', '594', '5163', '314', '2323', '2330', '2443', '2088', '2666', '4495', '5333', '5434', '5508', '3548', '318', '848', '5273', '6280', '1018', '1417', '1542', '2303', '6142', '320', '1755', '1785', '1885', '3704', '5511', '6388', '321', '517', '2387', '1875', '2071', '3219', '4321', '4471', '4676', '4801', '5000', '5277', '5534', '5714', '2651', '4951', '5395', '326', '4212', '5725', '2583', '3235', '4201', '614', '2941', '4254', '6005', '6112', '331', '1020', '2564', '2637', '2953', '3038', '3327', '3552', '6369', '577', '1596', '3855', '6116', '6274', '336', '718', '735', '1017', '1040', '1115', '3090', '5066', '5824', '5910', '6121', '337', '2267', '2350', '2100', '2150', '2733', '549', '1082', '1120', '2615', '3466', '3834', '5549', '5675', '5728', '341', '404', '1774', '2312', '2565', '2784', '2884', '4251', '4255', '5029', '5584', '895', '3124', '3890', '343', '5255', '5185', '505', '1111', '3312', '3486', '4595', '4950', '5450', '5637', '6104', '2492', '6118', '6149', '1717', '5891', '664', '4203', '4709', '5460', '5665', '2240', '6059', '6371', '2847', '705', '1784', '5883', '588', '605', '4812', '4372', '3257', '4678', '360', '963', '1732', '2979', '5191', '5343', '5787', '5684', '446', '550', '680', '1695', '2875', '3037', '3448', '3850', '5468', '6410', '1778', '2373', '2664', '4726', '422', '457', '1414', '3550', '3617', '3705', '5974', '365', '1992', '2963', '3049', '3320', '3637', '4866', '5028', '5308', '6030', '1622', '1758', '2687', '2775', '3304', '3447', '3484', '4227', '5354', '2998', '411', '1453', '1854', '1876', '1956', '3422', '3765', '3768', '3800', '4663', '5150', '5355', '1987', '2789', '3103', '4269', '4346', '2313', '2814', '5507', '1173', '2368', '3708', '4587', '5706', '6080', '2917', '3158', '3425', '5708', '5961', '5270', '3271', '3700', '3456', '3471', '382', '2361', '5968', '2973', '4389', '4775', '6016', '1808', '2920', '3571', '3806', '4025', '5944', '6368', '386', '657', '2497', '2636', '3009', '3044', '388', '2304', '2550', '2587', '3285', '4054', '1015', '1667', '1724', '2250', '3010', '3385', '5345', '5782', '6157', '391', '610', '4442', '4976', '4985', '5161', '1888', '2157', '2838', '3647', '4744', '5098', '5452', '5810', '5865', '6108', '6313', '628', '396', '5633', '397', '3457', '3710', '5033', '4124', '759', '2198', '3886', '5621', '1928', '1858', '2634', '2816', '1581', '1723', '2083', '2428', '4117', '5805', '2160', '3677', '4650', '3027', '1494', '4628', '4778', '4928', '6390', '1043', '1258', '3518', '4085', '4607', '485', '3375', '4777', '5958', '419', '1185', '2007', '2890', '3248', '3999', '4748', '4651', '5615', '1454', '2073', '2354', '4859', '6019', '882', '927', '1423', '426', '529', '653', '1232', '2159', '2400', '2442', '3712', '4427', '2271', '2437', '3239', '3387', '694', '2785', '3787', '2726', '3757', '4527', '4874', '1810', '1812', '1740', '2927', '5844', '434', '2380', '4892', '2739', '3177', '5229', '5818', '2767', '3182', '1815', '4745', '2355', '5041', '3054', '4378', '2408', '3885', '3719', '3771', '552', '785', '1359', '2055', '2139', '3497', '3577', '3586', '4945', '3960', '453', '1504', '2298', '2500', '454', '6165', '6176', '1047', '3261', '922', '4793', '4445', '4873', '2594', '2996', '3247', '4358', '2936', '4847', '5364', '5437', '1253', '2694', '1924', '5289', '1078', '2132', '2894', '6072', '3204', '473', '3612', '4262', '6062', '6189', '3127', '951', '1127', '6213', '479', '1548', '5011', '5687', '5491', '3245', '4266', '1102', '2578', '2851', '3068', '3071', '3163', '486', '1029', '2863', '4306', '1771', '5022', '5334', '5991', '6186', '516', '3132', '3146', '4893', '5554', '6377', '2111', '1335', '3232', '4672', '728', '1300', '1456', '1757', '3014', '5856', '6447', '6241', '2805', '3100', '3570', '3575', '4088', '5115', '501', '998', '1327', '2030', '2641', '5797', '2773', '1701', '4430', '2697', '3682', '510', '2434', '2447', '666', '5139', '512', '565', '2994', '3153', '4783', '5080', '3496', '6164', '2799', '5174', '5900', '6066', '2836', '3505', '3644', '3956', '4065', '1480', '3174', '6453', '1046', '2645', '5803', '6004', '3290', '5238', '2274', '4125', '4556', '5190', '2165', '3096', '5203', '5495', '1341', '3663', '6479', '1166', '1935', '639', '2650', '3258', '3395', '3499', '6203', '533', '1108', '651', '1360', '4537', '4900', '3504', '6006', '3516', '1983', '1960', '5072', '5263', '5796', '3488', '3686', '3809', '3226', '6367', '6400', '1224', '1460', '2418', '4802', '5929', '6036', '5375', '841', '4303', '1794', '3025', '3296', '5789', '4986', '1506', '5562', '5775', '5170', '5769', '564', '806', '2136', '2668', '3646', '5209', '1836', '1837', '568', '5337', '1798', '2282', '2514', '3846', '3766', '5051', '5093', '3282', '6475', '1750', '3578', '4158', '1851', '2815', '574', '3225', '5819', '2377', '3687', '2066', '4318', '5396', '3272', '1646', '581', '582', '2582', '1343', '2051', '4151', '2704', '4606', '5451', '5898', '5933', '887', '6127', '3829', '1790', '5388', '2713', '3092', '5794', '2742', '3187', '5640', '6051', '6130', '1678', '6053', '4570', '3145', '641', '3227', '3120', '601', '1191', '2518', '6402', '602', '958', '6178', '1616', '1735', '2471', '4687', '2593', '3200', '3788', '2835', '3815', '4180', '6466', '5740', '5994', '6120', '2228', '1039', '2802', '4166', '4790', '2309', '4633', '6099', '6068', '5772', '2804', '2876', '5879', '3259', '3454', '3713', '5015', '3772', '2105', '4199', '6197', '6418', '879', '1072', '2376', '2409', '2496', '2656', '5580', '5922', '652', '934', '2371', '2520', '2290', '2390', '2393', '2401', '2916', '6148', '2506', '4301', '5160', '5525', '5573', '2213', '3648', '4766', '725', '1963', '3111', '3675', '3744', '6437', '3190', '1827', '1880', '2568', '2709', '3403', '682', '5467', '1656', '1666', '3076', '3424', '687', '2649', '3659', '801', '5374', '936', '2531', '5779', '4557', '5638', '1917', '3721', '5314', '6277', '4841', '5832', '6063', '5145', '6101', '1871', '3241', '5816', '5839', '6092', '2911', '6422', '1218', '4621', '4963', '2536', '3052', '3283', '4133', '4436', '1949', '5197', '3350', '3994', '6468', '1375', '2892', '5663', '6082', '1916', '2975', '5815', '6094', '1818', '2104', '2737', '3495', '4652', '1679', '1883', '6074', '2017', '1277', '6218', '6097', '3336', '3430', '3511', '3557', '3716', '1607', '4753', '4282', '5228', '1310', '4616', '5669', '766', '2551', '5431', '2824', '3446', '2338', '774', '777', '3515', '1229', '2433', '5336', '1629', '964', '4063', '5517', '793', '5297', '889', '5904', '2056', '1061', '803', '4699', '5122', '6096', '3740', '6398', '2912', '4048', '810', '4702', '5390', '814', '4283', '4534', '822', '2809', '3277', '4042', '4507', '4786', '6389', '1860', '4387', '5700', '1514', '2969', '835', '1962', '2108', '3510', '2341', '5808', '5867', '5842', '4767', '3672', '4545', '853', '6365', '1366', '1658', '1564', '2881', '5483', '1909', '868', '3419', '3799', '871', '3970', '4878', '2490', '5146', '880', '6043', '5770', '3376', '3692', '885', '2848', '3253', '3135', '4686', '2858', '4712', '902', '1252', '3835', '1179', '1832', '2907', '5357', '6033', '6125', '908', '2315', '2370', '5418', '2235', '5864', '2698', '3781', '5445', '5791', '5966', '920', '2191', '5551', '3615', '929', '4720', '6088', '2285', '4888', '5822', '4615', '3877', '3907', '4163', '2705', '5518', '5149', '2314', '2413', '5804', '3780', '5752', '955', '6010', '1688', '2723', '6433', '6184', '1923', '2397', '3717', '5694', '5624', '1930', '2655', '3143', '3379', '3164', '6311', '2384', '2631', '1865', '995', '3250', '5522', '5989', '3007', '3061', '1001', '1974', '2110', '1002', '4248', '6086', '2595', '4226', '2530', '3012', '5964', '1008', '2219', '4380', '1857', '2227', '1016', '4462', '3207', '3603', '5031', '3579', '5641', '5634', '5766', '2676', '1056', '2545', '2286', '1418', '2417', '2499', '5959', '2837', '3754', '1077', '2013', '3167', '5950', '5346', '5930', '1830', '5097', '1092', '2537', '2404', '3492', '2627', '2280', '1125', '4746', '3269', '2048', '2750', '6050', '2147', '2715', '1753', '1783', '1807', '1861', '5448', '6007', '5129', '1640', '5045', '1187', '5997', '3818', '4139', '1964', '5926', '6111', '4617', '1210', '2239', '2369', '2247', '2192', '6022', '3537', '4271', '6254', '4871', '2657', '6198', '4297', '3338', '3917', '1270', '5836', '5841', '5892', '5951', '6206', '6408', '4019', '4520', '5401', '4455', '2092', '1991', '1290', '4454', '3067', '5100', '3747', '1770', '1309', '3924', '1323', '5978', '3452', '3724', '5279', '6249', '4404', '5759', '5886', '2133', '4697', '3896', '3026', '4754', '2300', '1367', '5240', '2353', '2367', '3957', '5724', '2223', '4566', '5828', '5936', '1400', '3133', '1651', '3953', '5653', '3609', '5885', '1562', '4816', '2319', '2436', '6085', '1776', '1833', '1859', '2513', '3229', '5367', '5715', '2257', '6167', '4001', '1475', '1553', '2026', '3862', '6200', '2230', '2523', '2854', '2295', '2383', '4721', '4682', '2576', '1509', '3882', '5199', '1536', '2707', '6227', '4530', '6183', '4223', '6129', '5171', '2867', '3082', '5670', '5906', '4608', '5620', '3409', '4376', '5614', '1603', '5953', '5746', '5831', '1636', '2220', '4525', '6404', '6048', '2452', '1650', '5932', '5960', '5745', '3519', '2060', '2661', '3077', '1680', '1698', '3657', '3998', '1934', '2065', '1706', '5771', '5955', '5980', '2756', '6376', '5988', '5813', '2929', '5489', '6181', '3353', '1752', '1754', '1811', '1822', '4502', '1824', '3630', '6463', '3360', '3853', '1772', '2441', '2472', '2511', '2542', '3019', '1795', '2967', '1806', '5476', '2671', '4136', '6145', '2281', '3255', '2812', '1879', '3861', '3926', '3243', '1899', '1919', '5798', '4181', '1905', '2010', '1908', '5389', '6038', '6331', '5315', '4209', '3421', '6220', '2540', '4334', '1943', '5531', '4937', '5153', '3730', '1957', '2244', '1958', '1961', '2023', '2117', '2173', '2214', '4509', '6434', '2182', '1966', '3545', '3580', '5378', '1969', '4279', '1973', '4322', '5291', '2663', '3783', '1980', '2658', '5692', '2106', '2232', '4459', '5721', '1993', '2029', '2527', '6244', '2018', '6076', '2020', '2021', '5201', '5237', '4795', '5147', '2151', '2027', '4480', '3555', '2049', '5774', '4403', '5674', '2057', '5281', '5372', '4331', '2081', '5371', '2087', '4347', '2094', '2101', '5869', '4770', '4102', '2121', '2137', '5218', '2398', '2652', '4267', '4603', '2170', '4392', '2199', '2633', '3294', '5421', '2217', '3944', '5852', '2243', '4952', '2524', '2251', '6259', '2399', '3899', '4051', '4106', '2365', '2276', '2591', '2470', '2921', '2347', '2491', '4149', '2328', '2374', '5845', '2346', '2539', '5873', '2358', '2474', '2372', '2375', '3148', '2392', '2395', '2577', '5501', '3064', '6131', '2432', '6020', '4027', '2438', '2453', '6113', '2487', '2543', '5838', '2903', '6160', '3581', '2606', '3653', '3525', '3743', '4344', '3963', '5196', '3118', '4760', '2626', '5138', '2966', '2684', '3535', '6228', '5368', '5762', '3356', '4815', '3568', '2719', '5462', '6009', '5506', '2759', '5778', '5942', '3472', '2783', '2790', '5124', '5049', '3909', '4216', '3945', '4069', '2943', '2908', '2910', '3534', '3725', '6107', '5404', '3371', '2965', '5513', '4987', '5847', '6089', '6026', '4049', '3070', '5990', '3695', '5954', '6287', '3309', '4575', '6084', '3220', '3228', '3481', '6255', '4130', '3895', '4862', '5601', '5738', '5876', '5921', '6025', '5729', '3884', '5945', '4479', '3289', '3313', '3366', '3381', '3416', '3433', '4809', '3509', '3524', '4046', '5734', '6081', '5109', '3738', '3748', '4929', '5719', '3770', '6163', '3831', '5925', '5660', '3859', '3865', '3866', '3867', '4973', '5213', '3869', '3878', '3874', '4093', '3879', '3887', '3904', '4050', '3905', '3923', '4033', '3931', '4143', '3934', '3991', '3936', '4078', '4095', '5814', '4005', '4013', '4028', '4041', '4122', '4043', '4128', '4081', '4177', '4094', '4146', '4150', '4101', '5545', '5928', '5736', '4337', '4831', '4369', '4598', '4384', '5776', '6351', '5492', '5173', '4584', '5393', '4637', '4656', '5316', '4752', '5806', '4776', '6232', '6014', '6057', '6150', '5195', '5259', '5711', '5408', '5710', '5382', '6041', '5587', '5911', '5578', '5702', '5712', '5952', '6035', '6201', '5757', '5761', '6046', '5941', '5947', '6011', '5833', '6047', '5972', '5855', '5993', '5890', '6105', '5916', '5995', '5918', '6278', '6045', '6476'))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G1.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点 2 存在于图中。\n"
     ]
    }
   ],
   "source": [
    "node_to_check = '2'\n",
    "\n",
    "if node_to_check in G2.nodes:\n",
    "    print(f\"节点 {node_to_check} 存在于图中。\")\n",
    "else:\n",
    "    print(f\"节点 {node_to_check} 不存在于图中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历边\n",
    "for edge in G2.edges:\n",
    "    source_node, target_node = edge\n",
    "    if source_node == node_to_check or target_node == node_to_check:\n",
    "        print(f\"节点 {node_to_check} 存在于边 {edge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典中的键都是唯一的\n"
     ]
    }
   ],
   "source": [
    "def has_duplicate_keys(dictionary):\n",
    "    seen = set()\n",
    "    for key in dictionary:\n",
    "        if key in seen:\n",
    "            return True\n",
    "        seen.add(key)\n",
    "    return False\n",
    "\n",
    "result = has_duplicate_keys(id2idx2)\n",
    "\n",
    "if result:\n",
    "    print(\"字典中存在重复的键\")\n",
    "else:\n",
    "    print(\"字典中的键都是唯一的\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_feats = source_dataset.features\n",
    "target_feats = target_dataset.features\n",
    "\n",
    "if source_feats is None:\n",
    "    source_feats = np.zeros((len(source_dataset.G.nodes()), 1))\n",
    "    target_feats = np.zeros((len(target_dataset.G.nodes()), 1))\n",
    "\n",
    "for i in range(len(source_feats)):\n",
    "    if source_feats[i].sum() == 0:\n",
    "        source_feats[i, -1] = 1\n",
    "for i in range(len(target_feats)):\n",
    "    if target_feats[i].sum() == 0:\n",
    "        target_feats[i, -1] = 1\n",
    "if source_feats is not None:\n",
    "    source_feats = torch.FloatTensor(source_feats)\n",
    "    target_feats = torch.FloatTensor(target_feats)\n",
    "    source_feats = source_feats.cuda()\n",
    "    target_feats = target_feats.cuda()\n",
    "source_feats = F.normalize(source_feats)\n",
    "target_feats = F.normalize(target_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_gt = graph_utils.load_gt(groundtruth,format='dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_gt = list(groundtruth_gt.keys())\n",
    "target_gt = list(groundtruth_gt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'in_degree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m in_degrees \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(source_dataset\u001b[39m.\u001b[39;49mG\u001b[39m.\u001b[39;49min_degree(source_gt))\n\u001b[1;32m      2\u001b[0m out_degrees \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(source_dataset\u001b[39m.\u001b[39mG\u001b[39m.\u001b[39mout_degree(source_gt))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'in_degree'"
     ]
    }
   ],
   "source": [
    "in_degrees = dict(source_dataset.G.in_degree(source_gt))\n",
    "out_degrees = dict(source_dataset.G.out_degree(source_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_A_hat, _ = Laplacian_graph(source_dataset.get_adjacency_matrix())\n",
    "target_A_hat, _ = Laplacian_graph(target_dataset.get_adjacency_matrix())\n",
    "source_feats = source_dataset.features\n",
    "target_feats = target_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 3903, 3904, 3905])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3906)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    The GCN multistates block\n",
    "    \"\"\"\n",
    "    def __init__(self, activate_function, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        activate_function: Tanh\n",
    "        input_dim: input features dimensions\n",
    "        output_dim: output features dimensions\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        if activate_function is not None:\n",
    "            self.activate_function = get_act_function(activate_function)\n",
    "        else:\n",
    "            self.activate_function = None\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        init_weight(self.modules(), activate_function)\n",
    "    \n",
    "    def forward(self, input, A_hat):\n",
    "        output = self.linear(input)\n",
    "        output = torch.matmul(A_hat, output)\n",
    "        if self.activate_function is not None:\n",
    "            output = self.activate_function(output)\n",
    "        return output\n",
    "\n",
    "def get_act_function(activate_function):\n",
    "    \"\"\"\n",
    "    Get activation function by name\n",
    "    :param activation_fuction: Name of activation function \n",
    "    \"\"\"\n",
    "    if activate_function == 'sigmoid':\n",
    "        activate_function = nn.Sigmoid()\n",
    "    elif activate_function == 'relu':\n",
    "        activate_function = nn.ReLU()\n",
    "    elif activate_function == 'tanh':\n",
    "        activate_function = nn.Tanh()\n",
    "    else:\n",
    "        return None\n",
    "    return activate_function\n",
    "\n",
    "def init_weight(modules, activation):\n",
    "    \"\"\"\n",
    "    Weight initialization\n",
    "    :param modules: Iterable of modules\n",
    "    :param activation: Activation function.\n",
    "    \"\"\"\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Linear):\n",
    "            if activation is None:\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data) #, gain=nn.init.calculate_gain(activation.lower()))\n",
    "            else:\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain(activation.lower()))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data = init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCNs=[]\n",
    "activate_function=\"Tanh\"\n",
    "input_dim = 538\n",
    "output_dim = 200\n",
    "GCNs.append(GCN(activate_function, input_dim, output_dim))\n",
    "input_dim = GCNs[-1].output_dim\n",
    "GCNs = nn.ModuleList(GCNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(G, id2idx):\n",
    "    edges1 = [(id2idx[n1], id2idx[n2]) for n1, n2 in G.edges()]\n",
    "    edges2 = [(id2idx[n2], id2idx[n1]) for n1, n2 in G.edges()]\n",
    "    \n",
    "    edges = edges1 + edges2\n",
    "    edges = np.array(edges)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_augmentation(self, dataset, type_aug='remove_edges'):\n",
    "        \"\"\"\n",
    "        Generate small noisy graph from original graph\n",
    "        :params dataset: original graph\n",
    "        :params type_aug: type of noise added for generating new graph\n",
    "        \"\"\"\n",
    "        edges = dataset.get_edges()\n",
    "        adj = dataset.get_adjacency_matrix()\n",
    "        \n",
    "        if type_aug == \"remove_edges\":\n",
    "            num_edges = len(edges)\n",
    "            num_remove = int(len(edges) * self.args.noise_level)\n",
    "            index_to_remove = np.random.choice(np.arange(num_edges), num_remove, replace=False)\n",
    "            edges_to_remove = edges[index_to_remove]\n",
    "            for i in range(len(edges_to_remove)):\n",
    "                adj[edges_to_remove[i, 0], edges_to_remove[i, 1]] = 0\n",
    "                adj[edges_to_remove[i, 1], edges_to_remove[i, 0]] = 0\n",
    "        elif type_aug == \"add_edges\":\n",
    "            num_edges = len(edges)\n",
    "            num_add = int(len(edges) * self.args.noise_level)\n",
    "            count_add = 0\n",
    "            while count_add < num_add:\n",
    "                random_index = np.random.randint(0, adj.shape[1], 2)\n",
    "                if adj[random_index[0], random_index[1]] == 0:\n",
    "                    adj[random_index[0], random_index[1]] = 1\n",
    "                    adj[random_index[1], random_index[0]] = 1\n",
    "                    count_add += 1\n",
    "        elif type_aug == \"change_feats\":\n",
    "            feats = np.copy(dataset.features)\n",
    "            num_nodes = adj.shape[0]\n",
    "            num_nodes_change_feats = int(num_nodes * self.args.noise_level)\n",
    "            node_to_change_feats = np.random.choice(np.arange(0, adj.shape[0]), num_nodes_change_feats, replace=False)\n",
    "            for node in node_to_change_feats:\n",
    "                feat_node = feats[node]\n",
    "                feat_node[feat_node == 1] = 0\n",
    "                feat_node[np.random.randint(0, feats.shape[1], 1)[0]] = 1\n",
    "            feats = torch.FloatTensor(feats)\n",
    "            if self.args.cuda:\n",
    "                feats = feats.cuda()\n",
    "            return feats\n",
    "        new_adj_H, _ = Laplacian_graph(adj)\n",
    "        if self.args.cuda:\n",
    "            new_adj_H = new_adj_H.cuda()\n",
    "        return new_adj_H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bowen': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "184224c1b2f199ca9ca1433de061991be7c32086fc5310f6aaf7f57995353458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
